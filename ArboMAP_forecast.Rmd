---
params:
  ## week to run forecast for
  forecast_date: "2018-08-15"
  ## state
  state_name: "South Dakota"
  state_code: "SD"
  ## predictors 
  predictor_var1: "tmeanc"
  predictor_var2: "vpd"
  ## mosquito settings
  mosquito_model: "stratifiedMII" # "simpleratio", "AUC", "MIGR", "MII", "stratifiedMIGR", "stratifiedMII"
  mosquito_doy_start: 140
  mosquito_doy_end: 214
  ## input data file locations
  file_human: !r file.path("data_human", "simulated_human_data.csv")
  file_mosquito: !r file.path("data_mosquito", "simulated_mosquito_data.csv")
    #if no strata, set to ""
  file_strata: !r file.path("data_strata", "example_strata_SD.csv")
    #if do not have sf object of counties saved as an RDS file, 
      #temporarily set to "create" once, and it will make the appropriate state file. Must have internet access.
    #if do not want to cache, 
      #set to "always_download" and it will download tigris shapefile each time. Must have internet access.
  file_district_sf: !r file.path("data_spatial", "sd_counties.RDS")
  #dev, limited models <<>>
  #file_models: !r file.path("data_models", "models.txt")
  file_models: !r file.path("dev", "models_tpfx.txt")
  folder_weather: "data_weather"
  ## data range settings
  #which years of human data to use
  year_human_start: 2004
  year_human_end: 2017
  #which years of mosquito data to use
  year_mosquito_start: 2012
  year_mosquito_end: 2018
  #which years of weather data to use
  year_weather_start: 2000
  year_weather_end: 2018
  #which years to include in modeling results
  year_modeling_start: 2004
  year_modeling_end: 2018
  #which years to show as comparison years in graphs
  year_compare_vis1: 2012
  year_compare_vis2: 2017
  ## additional settings
  # create appendix at end with more details and graphs 
  create_appendix: TRUE
  # length (days) of weather data to include in lags
  lag_length: 121
  # remove temporal outliers from human cases
  case_trim_alpha: 0.02
  # developer settings
  dev_settings: !r list()

title: "ArboMAP: Arbovirus Modeling and Prediction   \nto Forecast Mosquito-Borne Disease Outbreaks"
author: "Summary of Model Outputs (v3.1) for `r params$state_name`, `r params$forecast_date`  \nDawn M. Nekorchuk, Justin K. Davis, and Michael C. Wimberly  \n(mcwimberly@ou.edu)  \nGeography and Environmental Sustainability, University of Oklahoma"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r knitr_setup, include=FALSE}
options(warn=-1)
```

```{r libraries, include=FALSE}

#make sure pacman is installed
if (!require("pacman")) install.packages("pacman"); library(pacman)

#load packages, install if not installed
pacman::p_load(
  #data processing, tidyverse related 
  dplyr, readr, tidyselect, rlang, 
  tibble, stringr, glue,
  #add'l data processing
  zoo, 
  #modeling
  mgcv, splines,
  #spatial, maps and graphs
  tigris, sf, ggplot2,
  #report generation and interface
  knitr, shiny)

#Must use recent version of readr
if (packageVersion("readr") < 2.1){
  install.packages("readr")
}

#Single use functions are found in their section 
# for convenience when reviewing/editing code
# Most are found in the 'data_id_fields' section

```

```{r dev_parameters, include=FALSE}
#input is named list

#parameters available:

# save_model: TRUE/FALSE: Will create a list of saved model objects (using rest of input params)
# model_cached: must be named list of model objects, named from modeling file to pattern match
# model_cached_file: "no cached" <<>>????

# human_data: tbl of human case data (overrides file_human)
# mosquito_data: tbl of mosquito pool data (overrides file_mosquito)
# stratification_data: tbl of strata (overrides file_stratification)
# weather_data: tbl of weather data (overrides folder_weather & processing)
# district_sf: sf object of counties/districts for state (overrides file_district_sf)

# models_to_run: model formulas to run (overrides file_models)

# reg_function: "GAM" (hook for future possibility)

# resampling mosquito data before mosquito model
# NOTE: resampling code is mostly left as v3, 
#       and is likely inoperable unless field names are different 
#       in mosquito file that needs resampling.
#resample_mosquito: TRUE/FALSE
#resample_file: path and file


#Set up data objects 
# will use as tests for (possible) file loading in following data_load section
data_human <- NULL
data_mosquito <- NULL
data_strata <- NULL
data_weather <- NULL
data_sf_orig <- NULL
model_formulas <- NULL

#<<>> do overrides. if (length(params$dev_settings > 0). chk epidemiar dev settings DEV

#set defaults hard until dev settings <<>>

resample_mosquito <- FALSE
reg_function <- "GAM"
impute_human_missing_districts <- FALSE

```

```{r data_load, include=FALSE}
#Loads data from file locations given in parameters
#Note: does NOT do any data checks

if (is.null(data_human)){
  data_human <- readr::read_csv(params$file_human, 
                                show_col_types = FALSE)
}

if (is.null(data_mosquito)){
  data_mosquito <- readr::read_csv(params$file_mosquito, 
                                   show_col_types = FALSE)
}

if (is.null(data_strata)){
  #if given a strata file, which is optional
  if (!params$file_strata == ""){
    data_strata <- readr::read_csv(params$file_strata, 
                                   show_col_types = FALSE)
  }
}

if (is.null(data_sf_orig)){
  
  if (params$file_district_sf == "create"){
    #if user set to "create" then we will download tigris shapefile and save for future use
    
    #download tigris, internet required
    data_sf_orig <- tigris::counties(state = params$state_code, cb = TRUE)
    #save out for use next time
    #make folder if does not exist (if exists, just shows warning, suppressed)
    dir.create("data_spatial", showWarnings = FALSE)
    saveRDS(data_sf_orig, file.path("data_spatial", paste0(params$state_code, "_counties.RDS")))
    
  } else if (params$file_district_sf == "always_download"){
    #if "always_download" then we will download tigris shapefile each time (no save), internet required
    
    data_sf_orig <- tigris::counties(state = params$state_code, cb = TRUE)
    
  } else {
    #read in file from params
    
    data_sf_orig <- readRDS(params$file_district_sf)
    
  }
}#end is.null



if (is.null(model_formulas)){
  models_raw <- readr::read_csv(params$file_models, 
                                show_col_types = FALSE, quote = "\"")
  #create named list from tbl
  model_formulas <- models_raw %>% 
    dplyr::pull(2, name = 1)
  #just the model names for later use
  model_names <- names(model_formulas)
}

# Weather data
# Raw read in with prep for taking most recent value for day
# See data_weather_latest block for that processing

# Reading in of data & file modified time
#get list of csv files (NOT in subfolders)
env_csv_files_raw <- list.files(path = params$folder_weather, 
                                pattern="*.csv$",
                                full.names = TRUE, recursive = FALSE)

#keep the names of only csv files that are not empty
#not likely relevant here, however does no harm to check
file_condition <- sapply(env_csv_files_raw, function(x) {length(readr::count_fields(x, readr::tokenizer_csv())) > 1})
env_csv_files <- env_csv_files_raw[file_condition]

#read in all data files, and add the time the file was last modified
data_env_raw <- env_csv_files %>% 
  lapply(function(x) {
    readr::read_csv(x, show_col_types = FALSE) %>% 
      #add last modified time
      dplyr::mutate(file_time = file.info(x)$mtime)}) %>% 
  #bind list items into one dataset
  dplyr::bind_rows()


#Record row count of data as read in
dx_mosq_nrow_0load <- nrow(data_mosquito)
dx_human_nrow_0load <- nrow(data_human)
dx_env_nrow_0load <- nrow(data_env_raw) #note PRE de-duplication


```

```{r data_id_fields, echo=FALSE} 

#ID fields:
# If FIPS field is in all, will use fips (FULL 5 character version)
# Else use original district/county name matching
# Accepted field names here, there will be preferred, 
# but this gives some flexibility 
# Processing happens after read in, will create "arbo_ID" used afterwards
# Processing includes wrangling fips to match across all files
# Each list in DESCENDING order of priority
#   will only take the field that appears first
field_fips_accepted <- c("fips", "FIPS", "fips_code", "FIPS_CODE")
field_names_accepted <- c("district", "county")


#ID Functions
confirm_id_fields <- function(fld_vector){
  #Does any of the accepted fields (of a particular type)
  # exist in all 3 or 4 datasets
  # strata is OPTIONAL
  
  if (!is.null(data_strata)){
    #4 datasets
    
    my_count <- sum(
      any(fld_vector %in% names(data_human)),
      any(fld_vector %in% names(data_mosquito)),
      any(fld_vector %in% names(data_env_raw)),
      any(fld_vector %in% names(data_strata))
    )
    
    #true/false
    use_fld <- my_count == 4L
    
  } else {
    #3 datasets
    
    my_count <- sum(
      any(fld_vector %in% names(data_human)),
      any(fld_vector %in% names(data_mosquito)),
      any(fld_vector %in% names(data_env_raw))
    )
    
    #true/false
    use_fld <- my_count == 3L
    
  }
  
  return(use_fld)
  
} 

create_id_field <- function(my_tbl, fld_vector){
  #the field names in the dataset that match the accepted names
  # [[1]] takes the first
  field_to_copy <- intersect(fld_vector, names(my_tbl))[[1]]
  
  updated_tbl <- my_tbl %>% 
    #slight weirdness with dynamic field name in tidyverse on RHS
    #using glue {{}}
    #https://stackoverflow.com/questions/26003574/use-dynamic-name-for-new-column-variable-in-dplyr
    dplyr::mutate(arbo_ID = {{field_to_copy}})
}

standarize_fips <- function(my_tbl, fips_vector = field_fips_accepted){
  
  # Create new field arbo_ID which will be used for matching now on
  arbo_tbl <- create_id_field(my_tbl, fips_vector)
  
  # convert to standard 5 character (2 state + 3 county) format
  # if length 5, confirm/convert to character
  # if length 4, then full but read as number and state has leading 0
  #   convert to character, pad 0 in front
  # if length 3, confirm/convert to character, add state code
  # if length 2, then county code but read as number and county has leading 0
  #   convert to character, pad 0 in front to 3,
  #   then add state code
  
  #grab state code from shp 
  state_code <- data_sf_orig$STATEFP %>% unique()
  
  #<<>> DEV
  
}

simplifynames <- function(priornames=NULL) {
  
  #ORIGINAL name matching 
  
  # convert to lower case
  priornames <- tolower(priornames)
  
  # remove spaces
  priornames <- gsub(pattern=" ", replacement="", x=priornames, fixed=TRUE)
  
  # remove other offending placename modifiers
  priornames <- gsub(pattern="county", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="parish", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="par.", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="(zone)", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="lower", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="upper", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="southern", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="northern", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="saint", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="st", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern=".", replacement="", x=priornames, fixed=TRUE)
  
  # return names
  return(priornames)
  
}


standarize_names <- function(my_tbl, names_vector = field_names_accepted){
  
  #Create new field arbo_ID which will be used for matching now on
  arbo_tbl <- create_id_field(my_tbl, names_vector)
  
  #Use original name simplification
  arbo_tbl <- arbo_tbl %>% 
    dplyr::mutate(arbo_ID = simplifynames(arbo_ID))
}



# Set up arbo_ID 
if (confirm_id_fields(field_fips_accepted)){
  # use fips as arbo_ID
  
  data_human <- standarize_fips(data_human)
  data_mosquito <- standarize_fips(data_mosquito)
  data_env_raw <- standarize_fips(data_env_raw)
  
  if (!is.null(data_strata)){
    data_strata <- standarize_fips(data_strata)
  }
  
  #  data_sf_orig will be dealt with separately, as it is standard format
  #census shape: GEOID is 5 char FIPS code
  data_sf_orig <- data_sf_orig %>% 
    dplyr::mutate(arbo_ID = GEOID)
  
} else if (confirm_id_fields(field_names_accepted)){
  # use county names as arbo_ID
  
  data_human <- standarize_names(data_human)
  data_mosquito <- standarize_names(data_mosquito)
  data_env_raw <- standarize_names(data_env_raw)
  
  if (!is.null(data_strata)){
    data_strata <- standarize_names(data_strata)
  }
  
  
  #  data_sf_orig will be dealt with separately, as it is standard format
  #census shape: NAME is county name
  data_sf_orig <- data_sf_orig %>% 
    dplyr::mutate(arbo_ID = simplifynames(NAME))
  
}#end arbo_ID setup
```

```{r data_dates, echo=FALSE} 

# Dates in data
#   "date_obs" will become the standard date field name
# Filtering on years given in parameters
# Setting up week-related variables

# Note: tryCatch not helpful b/c of as.Date return values 
# > as.Date("2019-6-4")
# [1] "2019-06-04"
# > as.Date("2019-6-4", "%m/%d/%Y")
# [1] NA
# > as.Date("6/4/2019")
# [1] "0006-04-20"
# also note that tryCatch doesn't work inside mutate plain
# need rowwise or other solution to use that
# So below solution is not quite as robust as it could be, 
#  but it should be pretty good

data_human <- data_human %>% 
  #try old specified format first
  #gives NA when given dates like "2019-6-4"
  dplyr::mutate(date_obs = as.Date(date, format = "%m/%d/%Y"),
                #test for NA and let as.Date guess this time
                #MUST use ifelse, not if_else b/c that evaluates all
                # and as.Date will throw error if given the old format without pattern
                # however ifelse strips date format, so must cast it afterwards
                # note: zoo package used for default origin for as.Date()
                date_obs = as.Date(ifelse(is.na(date_obs), 
                                          as.Date(date), 
                                          date_obs)))

data_mosquito <- data_mosquito %>% 
  #try old specified format first
  #gives NA when given dates like "2019-6-4"
  dplyr::mutate(date_obs = as.Date(col_date, format = "%m/%d/%Y"),
                #test for NA and let as.Date guess this time
                #MUST use ifelse, not if_else b/c that evaluates all
                # and as.Date will throw error if given the old format without pattern
                # however ifelse strips date format, so must cast it afterwards
                # note: zoo package used for default origin for as.Date()
                date_obs = as.Date(ifelse(is.na(date_obs), 
                                          as.Date(col_date), 
                                          date_obs)))


## Filter data by year parameters
# More data may be present in the files than what we want to use 
#   e.g. incomplete year data
# Adds useful date parts as fields for use here and later
# Note: Calendar year is used for filtering
# <<>> EPIDATE DEV check if switch is correct
data_human <- data_human %>% 
  #year field for filtering
  dplyr::mutate(year_obs = lubridate::year(date_obs),
                #also doy, used later in modelling
                doy = as.numeric(format(date_obs, "%j")),
                #week of year DEV EPIDATE <<>>
                TBC_weeknum = as.numeric(format(date_obs, "%U"))) %>% 
  #filter year range from parameter input
  dplyr::filter(year_obs >= params$year_human_start & 
                  year_obs <= params$year_human_end)


data_mosquito <- data_mosquito %>% 
  #year field for filtering (here and later in mosq modelling)
  dplyr::mutate(year_obs = lubridate::year(date_obs),
                #also doy, used later in mosq modelling
                doy = as.numeric(format(date_obs, "%j"))) %>% 
  #filter year range from parameter input
  dplyr::filter(year_obs >= params$year_mosquito_start & 
                  year_obs <= params$year_mosquito_end) 
#Note: doy filtering happens in code block data_mosq_processing_dx below


data_env <- data_env %>% 
  #already has year field from GEE <<>> DEV document in user's guide data requirements
  #filter year range from parameter input
  dplyr::filter(year >= params$year_weather_start & 
                  year <= params$year_weather_end) 



## Dates for forecasts and functions

#week of forecast, given by user
date_forecast <- as.Date(params$forecast_date, "%Y-%m-%d")
date_forecast_week <- format(date_forecast, "%U") %>% as.numeric() #keeping this name, but flagging with TBC_

#<<>> EPIDATE DEV keeping old date processing for the moment until conversion to epiweeks  

# makes sure we round to the previous Sunday, so that this week is included
TBC_weekinquestionSun <- date_request - (as.numeric(strftime(date_request, '%u')) %% 7)

TBC_weekinquestionSat <- TBC_weekinquestionSun + 6
TBC_weekinquestionSunstr <- strftime(TBC_weekinquestionSun, '%A')
TBC_weekinquestionSatstr <- strftime(TBC_weekinquestionSat, '%A')

TBC_weekinquestionSundoy <- as.numeric(format(TBC_weekinquestionSun, "%j"))

# figure out which is the last Sunday in the max desired year
TBC_maxhumandesireddate <- as.Date(paste(params$year_modeling_end, "-12-31", sep=""))
TBC_maxhumandesireddate <- TBC_maxhumandesireddate - (as.numeric(strftime(TBC_maxhumandesireddate, '%u')) %% 7)
TBC_maxhumandesireddatestr <- strftime(TBC_maxhumandesireddate, '%A')

# figure out which is the first Sunday in the min desired human year
TBC_minhumandesireddate <- as.Date(paste(params$year_modeling_start, "-01-01", sep=""))
TBC_minhumandesireddate <- TBC_minhumandesireddate - (as.numeric(strftime(TBC_minhumandesireddate, '%u')) %% 7)
TBC_minhumandesireddatestr <- strftime(TBC_minhumandesireddate, '%A')


```

```{r data_human_processing_dx, echo=FALSE} 
## Human

#Record row count (in human_year_start through human_year_end)
dx_human_nrow_1range <- nrow(data_human)

#Clean human data
# remove any with unmatched district info
#   especially necessary for doing regression modeling
data_human <- data_human %>% 
  dplyr::filter(arbo_ID %in% unique(data_sf_orig$arbo_ID),
                #remove any without a good date
                (!is.na(date_obs)))

#Record row count post cleaning
dx_human_nrow_2clean <- nrow(data_human)

#gather list of districts in human data
dx_human_districts <- data_human %>% 
    pull(arbo_ID) %>% unique() %>% sort()


#[V3] DEV imputemissingdistricts was hard coded as FALSE in v3 production code
# updated names and code, and added dev option for renamed impute_human_missing_districts, default = FALSE
# DEV This looks like it samples existing districts as data for the missing districts.
#     I can't think of a time where this would be wanted. May be able to delete in future. [DMN]
if (impute_human_missing_districts){
  #list of districts in that state that are missing from human data
  districts_missing <- data_sf_orig$arbo_ID[!(data_sf_orig$arbo_ID %in% dx_human_districts)]
  #create random rows to fill in one observation for each missing district
  # by pulling a sampled row from the observed human data (for a different district) 
  sample_rows <- data_human %>% 
    dplyr::filter(row_number() %in% sample(x = 1:nrow(data_human),
                                           size = length(districts_missing),
                                           replace = TRUE))
  #pull only appropriate columns and make new tibble to add
  data_human_imputed <- tibble::tibble(arbo_ID = districts_missing) %>% 
    dplyr::bind_cols(sample_rows %>% 
                       dplyr::select(date_obs, year_obs, doy, TBC_weeknum)) 
  #bind imputed data to existing data
  data_human <- dplyr::bind_rows(data_human, data_human_imputed)
  
}


```

```{r data_mosq_processing_dx, echo=FALSE} 

## Mosquito
# Cleaning, filtering, statistics for report

#Record row count (in mosquito_year_start through mosquito_year_end)
dx_mosq_nrow_1range <- nrow(data_mosquito)

#Clean mosq data
# remove any with unmatched district info
#   especially necessary for doing regression modeling
#   Not original done in [V3] for simpleratio or AUC, 
#   but it makes sense to be consistent here. 
data_mosquito <- data_mosquito %>% 
  dplyr::filter(arbo_ID %in% unique(data_sf_orig$arbo_ID),
                #remove any with NAs in wnv_result or doy
                #both are needed in regression modeling
                #doy (and obs_year) would be na is date_obs was na
                (!is.na(wnv_result)),
                (!is.na(date_obs)))

#Record row count post cleaning
dx_mosq_nrow_2clean <- nrow(data_mosquito)

#Filter by mosquito range
data_mosquito <- data_mosquito %>% 
  #filter by doy
  dplyr::filter(doy >= params$mosquito_doy_start &
                  doy <= params$mosquito_doy_end)

#Record row count post doy filtering
dx_mosq_nrow_3filtered <- nrow(data_mosquito)

#gather list of districts in mosquito data
dx_mosq_districts <- data_mosquito %>% 
  pull(arbo_ID) %>% unique() %>% sort()
  
#max/latest year diagnostics and statistics for report text and debugging
# note this is being done after mosquito_year_end filtering
#  so not latest in data file, but latest in dataset for this forecast
data_mosq_maxyr <- data_mosquito %>% 
  dplyr::filter(year_obs == max(.$year_obs, na.rm = TRUE))
#number rows of data in max year
dx_mosq_nrow_maxyr <- nrow(data_mosq_maxyr)
#number of wnv positive pools in max year
mosq_pos_num_maxyr <- data_mosq_maxyr %>% 
  dplyr::filter(wnv_result == 1) %>% 
  nrow()
#percent of pools positive in max year
mosq_pos_perc_maxyr <- mosq_pos_num_maxyr / dx_mosq_nrow_maxyr * 100 %>% 
  round(3)
```

```{r mosquito_resampling, echo=FALSE} 

if (resample_mosquito){
  
  # read and process resampling file
  data_resample <- readr::read_csv(params$resample_file, show_col_types = FALSE)
  
  # added needed dates
  data_resample <- data_resample %>% 
    #try old specified format first
    #gives NA when given dates like "2019-6-4"
    dplyr::mutate(date_obs = as.Date(col_date, format = "%m/%d/%Y"),
                  #test for NA and let as.Date guess this time
                  #MUST use ifelse, not if_else b/c that evaluates all
                  # and as.Date will throw error if given the old format without pattern
                  # however ifelse strips date format, so must cast it afterwards
                  # note: zoo package used for default origin for as.Date()
                  date_obs = as.Date(ifelse(is.na(date_obs), 
                                            as.Date(col_date), 
                                            date_obs)),
                  # and doy
                  doy = as.numeric(format(date_obs, "%j")))
  
  # [V3] NOTE: Keeping original (v3) code with new names only
  # No test resample file to try
  # However, I believe it will fail as currently/previously coded, 
  #   unless mosquito data file has a different format when it needs resampling
  #   data_mosquito$total nor $positives does not exist?
  
  #create subset
  positivedoys <- data_resample$doy[data_resample$wnv_result == 1]
  negativedoys <- data_resample$doy[data_resample$wnv_result == 0]
  
  # create and fill up the expanded file
  wnv <- data.frame()
  for (i in 1:nrow(data_mosquito)) {
    
    positives <- data_mosquito$positives[i]
    negatives <- data_mosquito$total[i] - data_mosquito$positives[i]
    
    if (positives > 0) {
      
      tempdf1 <- data.frame(doy = sample(x=positivedoys,
                                         size=positives,
                                         replace=TRUE),
                            year = data_mosquito$year[i],
                            district = district_shapes$district[1], #? hard-coded first? DEV
                            wnv_result = 1)
      
    } else { tempdf1 <- data.frame() }
    
    if (negatives > 0) {
      
      tempdf0 <- data.frame(doy = sample(x=negativedoys,
                                         size=negatives,
                                         replace=TRUE),
                            year = data_mosquito$year[i],
                            district = district_shapes$district[1], #? hard-coded first? DEV
                            wnv_result = 0)
      
    } else { tempdf2 <- data.frame() }
    
    data_mosquito <- bind_rows(data_mosquito, tempdf1, tempdf0)
  }
  
} #end if resample
```

```{r mosquito_infection_model, echo=FALSE} 

# Calculates MIRsummarystat (mosquito infection rate), based on mosquito model
#   Creates mosq_mir : 
#   dataset with 'year_obs' and 'MIRsummarystat's, (and 'strata' for stratified models)
#   MIRsummarystat_raw : calculated via modelling
#   MIRsummarystat_ctr : post centering of above raw value
#   MIRsummarystat : NOT created in this block, but in combining data code block below.
#                   _ctr after imputation of any missing that are needed (used in modelling)

# If match failure, default will be AUC
# Note: If add a mosquito model, you MUST add it here
if (params$mosquito_model %in% c(
  "simpleratio", 
  "AUC",
  "MIGR",
  "MII",
  "stratifiedMIGR", 
  "stratifiedMII")){
  mosquito_model_clean <- params$mosquito_model
} else {
  #if unmatched, default AUC
  mosquito_model_clean <- "AUC"
}
#list of non/stratified models for easier filtering
mosq_nonstrat_models <- c("simpleratio", "AUC", "MIGR", "MII")
mosq_strat_models <- c("stratifiedMIGR", "stratifiedMII")

# Potentially multiple if blocks per model type, 
#   depends on processing/calculations needed

###
#Primary set of calculation blocks, includes all model types
if (mosquito_model_clean == "simpleratio") {
  
  mosq_mir <- data_mosquito %>% 
    #total of positive pools, total pools, per year
    group_by(year_obs) %>% 
    dplyr::summarise(tot_pos = sum(wnv_result, na.rm=TRUE),
                     tot_test = n()) %>%
    #simpleratio MIR is ratio of total positive pools over total pools tested
    dplyr::mutate(MIRsummarystat_raw = tot_pos / tot_test) %>% 
    #select only year and summary stat for consistency
    dplyr::select(year_obs, MIRsummarystat_raw)
  
} #end if 'simpleratio'

if (mosquito_model_clean == "AUC") {
  
  data_mosquito <- data_mosquito %>% 
    # #remove rows with NA
    # # [v3] dev note: I'm assuming glmer is unhappy otherwise [DMN]
    # dplyr::filter_at(vars(wnv_result, doy, year_obs),
    #                  #all variables listed must be not NA
    #                  all_vars(!is.na(.))) %>% 
    # [v3] create a variable that at least has a little chance of being orthogonal to 1.
    dplyr::mutate(dminus = doy - mean(.$doy, na.rm = TRUE),
                  #factor year for modeling
                  year_obs = factor(year_obs))
  
  #data frame for glmer
  data_mosq_df <- data_mosquito %>% as.data.frame()
  
  # [V3] run a random effect model on orthogonalized data
  mir_glm <- lme4::glmer(wnv_result ~ poly(dminus, 2) + (poly(dminus, 2)|year_obs),
                         family = binomial(),
                         data = data_mosq_df)
  
  # [V3] create a data frame to store the calculations for the aucs
  pred_frame <- expand.grid(year_obs = unique(data_mosquito$year_obs),
                            dminus = seq(from = min(data_mosquito$dminus, na.rm=TRUE),
                                         to = max(data_mosquito$dminus, na.rm=TRUE),
                                         length.out = 100))
  pred_frame$pred <- predict(mir_glm,
                             newdata = pred_frame,
                             type = "response")
  
  # calculate the AUCs
  # DEV note: Appears to be the sum of the predictions from all dminus values per year [DMN]
  mosq_mir <- pred_frame %>% 
    dplyr::group_by(year_obs) %>% 
    dplyr::summarise(MIRsummarystat_raw = sum(pred, na.rm = TRUE)) %>% 
    #make year numeric again (not factor)
    dplyr::mutate(year_obs = as.numeric(as.character(year_obs)))
  
  #make year numeric again (not factor)
  data_mosquito <- data_mosquito %>% 
    dplyr::mutate(year_obs = as.numeric(as.character(year_obs)))
  
}

if (mosquito_model_clean %in% c("MIGR", "MII")) {
  
  #<<>> DEV does arbo_ID really have to be a factor? is it used in these regressions??
  
  data_mosquito <- data_mosquito %>% 
    # [v3] create a variable that at least has a little chance of being orthogonal to 1.
    dplyr::mutate(dminus = doy - mean(.$doy, na.rm = TRUE),
                  #factor year for modeling
                  year_obs = factor(year_obs),
                  #arbo_ID (district|county|fips) to factor (for regression)
                  arbo_ID = factor(arbo_ID))
  
  #data frame for glmer
  data_mosq_df <- data_mosquito %>% as.data.frame()
  
  
  #[V3] run a random effect model on orthogonalized data
  mir_glm <- lme4::glmer(wnv_result ~ 1 + dminus +
                           (0+1|year_obs) +
                           (0+dminus|year_obs),
                         family = binomial(),
                         data = data_mosq_df)
  
  ##DEV : seemingly not used again in [V3], so temporarily removing to see if we need to keep it
  #data_mosq_df$est <- predict(data_mosq_df, newdata=wnv, type="response")
  #data_mosquito <- data_mosq_df %>% tibble::as_tibble()
  
  #make year numeric & arbo_id character again (not factor)
  data_mosquito <- data_mosquito %>% 
    dplyr::mutate(year_obs = as.numeric(as.character(year_obs)),
                  arbo_ID = as.character(arbo_ID))
  
  
  if (mosquito_model_clean == "MIGR") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(year_obs = rownames(nlme::random.effects(mir_glm)$year_obs) %>% 
                         as.numeric(),
                       #[,1] are intercept values [DMN]
                       MIRsummarystat_raw = nlme::random.effects(mir_glm)$year_obs[,1]) 
    
    
  }
  if (mosquito_model_clean == "MII") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(year_obs = rownames(nlme::random.effects(mir_glm)$year_obs) %>%
                         as.numeric(),
                       #[,2] are dminus values [DMN]
                       MIRsummarystat_raw = nlme::random.effects(mir_glm)$year_obs[,2]) 
    
  }
  
} #end if c("MIGR", "MII")

if (mosquito_model_clean %in% mosq_strat_models) {
  
  #add strata information to mosquito data
  data_mosquito <- data_mosquito %>% 
    dplyr::left_join(data_strata %>% 
                       select(arbo_ID, strata),
                     by = "arbo_ID") %>%
    #make sure all have a strata
    dplyr::filter(!is.na(strata)) %>% 
    # [v3] create a variable that at least has a little chance of being orthogonal to 1.
    dplyr::mutate(dminus = doy - mean(.$doy, na.rm = TRUE),
                  #create a stratum per year factor for regression
                  stratum_year = paste(strata, year_obs, sep = "_") %>% 
                    factor())
  
  #data frame for glmer
  data_mosq_df <- data_mosquito %>% as.data.frame()
  
  
  #[V3] run a random effect model on orthogonalized data
  mir_glm <- lme4::glmer(wnv_result ~ 1 + dminus +
                           (0+1|stratum_year) +
                           (0+dminus|stratum_year),
                         family = binomial(),
                         data = data_mosq_df)
  
  ##DEV : seemingly not used again in [V3], so temporarily removing to see if we need to keep it
  #data_mosq_df$est <- predict(data_mosq_df, newdata=wnv, type="response")
  #data_mosquito <- data_mosq_df %>% tibble::as_tibble()
  
  
  if (mosquito_model_clean == "stratifiedMIGR") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(stratum_year = rownames(nlme::random.effects(mir_glm)$stratum_year),
                       #[,1] seems to pull the intercept values [DMN]
                       MIRsummarystat_raw = nlme::random.effects(mir_glm)$stratum_year[,1]) %>% 
      #split stratum and year back out
      mutate(year_obs = stringr::str_split_fixed(stratum_year, "_", n = 2)[,2],
             strata = stringr::str_split_fixed(stratum_year, "_", n = 2)[,1],
             #convert year back to numeric
             year_obs = as.numeric(year_obs),
             #need to convert strata back to whatever data type as in data_strata
             #    in example is dbl for '101', etc. But could conceivably be numeric or string
             strata = as(strata, class(data_strata$strata))) %>% 
      #drop old field
      select(-stratum_year)
    
  }
  if (mosquito_model_clean == "stratifiedMII") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(stratum_year = rownames(nlme::random.effects(mir_glm)$stratum_year),
                       #[,2] seems to pull the dminus values [DMN]
                       MIRsummarystat_raw = nlme::random.effects(mir_glm)$stratum_year[,2]) %>% 
      #split stratum and year back out
      mutate(year_obs = stringr::str_split_fixed(stratum_year, "_", n = 2)[,2],
             strata = stringr::str_split_fixed(stratum_year, "_", n = 2)[,1],
             #convert year back to numeric
             year_obs = as.numeric(year_obs),
             #need to convert strata back to whatever data type as in data_strata
             #    in example is dbl for '101', etc. But could conceivably be numeric or string
             strata = as(strata, class(data_strata$strata))) %>% 
      #drop old field
      select(-stratum_year)
    
  }
  
} #end if c("stratifiedMIGR", "stratifiedMII")


# Center the MIR statistic 
# [V3] Originally done in the plotting code. Carried through to modeling, so done here. 
mosq_mir <- mosq_mir %>% 
  dplyr::mutate(MIRsummarystat_ctr = MIRsummarystat_raw - mean(.$MIRsummarystat_raw, na.rm = TRUE))


# # DEV testing output
# write_csv(mosq_mir, file = file.path("dev", "mosq_mir", paste0(mosquito_model_clean, "_postctr.csv")))
```

```{r data_weather_latest, echo=FALSE} 

# As we will likely have data from the same day in multiple files,
# we want to take only the LATEST value
# (post ID set up section to remove extraneous fields before processing)

data_env <- data_env_raw %>% 
  #trim to just needed data
  select("arbo_ID", "doy", "year", "file_time", 
         params$predictor_var1, params$predictor_var2) %>% 
  #create date field from year and doy
  dplyr::mutate(date_obs = as.Date(paste(year, doy, sep = "-"),
                                   "%Y-%j")) %>% 
  # Group by county and date, 
  group_by(arbo_ID, date_obs) %>% 
  # sort by modified time DESC, 
  arrange(desc(file_time)) %>% 
  # slice(1) to get first/latest (or only if not duplicated)
  slice(1) %>% 
  #drop file time
  dplyr::select(-file_time) %>%
  #ungroup to finish
  dplyr::ungroup()

#Note: This does not check for missing days (IMPLICIT missing)
# If adding, adapt code from function corral_environmental()
#  in data_corrals.R in epidemia_runspace (internal repo, Github)
# Explicit missing in handled in the anomalized section, 
#   these NAs are filled the weather model predicted values
```

```{r data_weather_anomalization, echo=FALSE} 

#Note: field names here are dynamically created 
#   based on user parameters for var1 and var2.
#   Tidyverse programming is a little weird with that 
#     (though the syntax is cleaner now than earlier versions).
# HOWEVER, as we are dealing with dataframes (not tibbles) for bam anyway
#   using base R and [V3] code

# Params format reminder: 
#  predictor_var1: "tmeanc"
#  predictor_var2: "vpd"

#data frame for bam
data_env_df <- data_env %>% 
  #ID must be a factor for modeling
  mutate(arbo_ID = factor(arbo_ID)) %>% 
  as.data.frame()

#loop to do each variable
for (v in c(params$predictor_var1, params$predictor_var2)) {
  
  #set up temp variable with known name (to use in bam call)
  # will be overwritten each loop
  data_env_df$this_var <- data_env_df[,v]
  
  #run a bam model with smooth on day of year (seasonal) and district
  this_env_mod <- mgcv::bam(this_var ~ arbo_ID + s(doy, bs="cc", by=arbo_ID), 
                            data = data_env_df, 
                            discrete = TRUE)
  
  #predicted values
  this_preds <- predict(this_env_mod, newdata = data_env_df)
  
  # any missing environmental data is filled in with predicted values [V3]
  #   this means we are updating our data, so we will have to overwrite data_env 
  #   note that this is POST env modelling, but PRE forecast modelling
  data_env_df[is.na(data_env_df[,v]), v] <- this_preds[is.na(data_env_df[,v])]
  
  #anomaly calculations: deviance between observed and predicted
  #   creating a dynamically generated name that is "anom_" plus the name of the variable
  data_env_df[,paste("anom_", v, sep="")] <- data_env_df[,v] - this_preds
  # then should there be any NA values, they are given the value 0 instead
  data_env_df[is.na(data_env_df[,paste("anom_", v, sep="")]), paste("anom_", v, sep="")] <- 0
  
}

#clean up and updating data_env
# changes were made of data_env_df that we need to keep
#   anomalized variables, imputed values
data_env <- data_env_df %>% 
  #convert table to tibble
  tibble::as_tibble() %>% 
  #drop temporary variable
  dplyr::select(-this_var)
#remove temporary table to avoid confusion
rm(this_preds)
#remove large temporary bam model
rm(this_env_mod)

```


```{r data_combine, echo=FALSE} 

# Human, mosq infection rate, and environmental data together
#   creating full matrix in prep for forecast modeling

# After creating frame based on human data, other data may need filling
#   so additional processing/calculations happen in this block

###
## Human

#determine the range (of weeks of year) we should include in modelling [V3]
# this is based on the human case_trim_alpha parameter
# this is removing outlier human cases that are very early or late in season [DMN]
#find min&max weeks using the case_trim_alpha parameter
human_wks_alpha <- quantile(data_human$TBC_weeknum, 
                            probs = c(params$case_trim_alpha/2, 
                                      1 - (params$case_trim_alpha/2)), na.rm = TRUE)
human_wk_min <- human_wks_alpha[1]
human_wk_max <- human_wks_alpha[2]


#Create dataset that will become the complete combined data for modelling input
#First, use human data (what we are predicting) to set start and end dates
# need to ensure that the end week is the last week of the year of the requested forecast week, 
#   so seq.Date is reverse chronological
weeks_model_list <- seq.Date(from = TBC_maxhumandesireddate, to = TBC_minhumandesireddate, by =-7)
# Create a frame that has entries for all district for all weeks in modelling list
data_combined_prep <- tidyr::expand_grid(arbo_ID = dx_human_districts,
                                         mod_wk_dt = weeks_model_list) %>% 
  #add other useful date fields
  dplyr::mutate(mod_wk_dt_yr = as.numeric(format(mod_wk_dt, "%Y")),
                mod_wk_dt_doy = as.numeric(format(mod_wk_dt, "%j")),
                TBC_mod_wk_dt_week = as.numeric(format(mod_wk_dt, "%U")),
                #create a flag if the week was in years between 
                #  params year_human_start through year_human_end
                observed = if_else(mod_wk_dt_yr >= params$year_human_start &
                                    mod_wk_dt_yr <= params$year_human_end,
                                   TRUE,
                                   FALSE),
                #create a flag if the week was within the trimmed alpha week range)
                modeled = if_else(TBC_mod_wk_dt_week >= human_wk_min &
                                    TBC_mod_wk_dt_week <= human_wk_max,
                                  TRUE,
                                  FALSE))

#count the number of cases that occur in each week for each district
#   note, if a district had no cases for that week, there will not be a row entry
data_human_week <- data_human %>% 
  dplyr::group_by(arbo_ID, year_obs, TBC_weeknum) %>% 
  #each row is a case
  dplyr::summarise(case_count = n(), .groups = "drop") %>% 
  #binary if any cases happened in that week in that district
  dplyr::mutate(any_cases = if_else(case_count >= 1,
                                    1, #yes, there was at least one case in this district-week 
                                    0))

#join, and fill 0s for any district-week that had no cases
data_combined_prep <- data_combined_prep %>% 
  left_join(data_human_week, by = c("arbo_ID", "mod_wk_dt_yr" = "year_obs", "TBC_mod_wk_dt_week" = "TBC_weeknum")) %>% 
  dplyr::mutate(case_count = tidyr::replace_na(case_count, 0),
                any_cases = tidyr::replace_na(any_cases, 0)) 


## Human case stats for report text

# Total number of cases over all districts over all time
state_cases_total <- sum(data_combined_prep$case_count, na.rm=TRUE)
state_any_cases_total  <- sum(data_combined_prep$any_cases, na.rm=TRUE)

# What percentage of cases we'll likely have seen before the start of this week
#   based on historical years
# DEV I believe the logic wasn't quite right in [V3]. [DMN]
#   I am adding a filter to remove the current year from the calculations, and
#   filtering by date (has been combined into weeks, but by date will yield same result here)  

cases_before_fc_wk <- data_combined_prep %>% 
  dplyr::filter(TBC_mod_wk_dt_week < date_forecast_week) %>% 
  pull(case_count) %>% 
  sum(na.rm = TRUE)

cases_perc_obs_hx <- 100 * (cases_before_fc_wk / state_cases_total) %>% round(2)


###
## Mosquito MIR 
# mosq_mir is name of dataset from mosquito modeling with MIRsummarystat
# MIRsummarystat_ctr : field with MIRsummarystat raw values that have been centered
# MIRsummarystat : created in this block, from _ctr, but also with imputation for needed missing entries

# Create temporary frames of full data, add in mir known, flag for imputation
#   separated out by stratification models, as that involves the strata column as well 
# Using data_combined_prep (the to-be-complete combined data)
# to create a data frame with all the years that we will need for modeling

if (mosquito_model_clean %in% mosq_nonstrat_models){

  #for NON-stratified mosq models
  
  prep_mir_filler <- data_combined_prep %>% 
    dplyr::select(mod_wk_dt_yr) %>% 
    #one entry for each year
    dplyr::distinct() %>% 
    #join with mosq_mir to get 'MIRsummarystat_'s
    dplyr::left_join(mosq_mir, by = "mod_wk_dt_yr") %>% 
    #add flag if MIR summary stat needs to be imputed (i.e. if that combination is missing)
    # AND if the year in within the human year range 
    dplyr::mutate(mir_impute = (is.na(MIRsummarystat_ctr) & 
                                     mod_wk_dt_yr >= params$year_human_start &
                                     mod_wk_dt_yr <= params$year_human_end))
  
} else if (mosquito_model_clean %in% mosq_strat_models){
  
  #for stratified mosq models
  
  prep_mir_filler <- tidyr::expand_grid(strata = data_strata %>% 
                                          dplyr::pull(strata) %>% 
                                          unique() %>% sort(),
                                        mod_wk_dt_yr = data_combined_prep %>% 
                                          dplyr::pull(mod_wk_dt_yr) %>% 
                                          #for consistency with data_combined_, 
                                          #keep with reverse chronological
                                          unique() %>% sort(decreasing = TRUE)) %>% 
    #join with mosq_mir to get 'MIRsummarystat_'s
    dplyr::left_join(mosq_mir, by = c("strata", "mod_wk_dt_yr" = "year_obs")) %>% 
    #add flag if MIR summary stat needs to be imputed (i.e. if that combination is missing)
    # AND if the year in within the human year range 
    dplyr::mutate(mir_impute = (is.na(MIRsummarystat_ctr) & 
                                     mod_wk_dt_yr >= params$year_human_start &
                                     mod_wk_dt_yr <= params$year_human_end))
}

#replace NA MIRsummarystat_ctr with zero (i.e. average risk, per comment from [V3])
# creating final MIRsummarystat variable to use in modelling
prep_mir_filler <- dplyr::mutate(prep_mir_filler, 
                                 MIRsummarystat = if_else(mir_impute, 
                                                          0,
                                                          MIRsummarystat_ctr))

# IF MIRsummarystat has been imputed, then doing exactfit
#   Add appropriate exactfit columns here

yrs_mir_imputed <- prep_mir_filler %>% 
  filter(mir_impute == TRUE) %>% 
  pull(mod_wk_dt_yr) %>% unique() %>% sort(decreasing = TRUE)

#loop over these years to add a column for each year
# that flags if it will be an exactfit
for (this_yr in yrs_mir_imputed){
  
  this_col_name <- paste0("exactfit_", this_yr)
  
  prep_mir_filler <- prep_mir_filler %>% 
    #slight weirdness for dynamic field name on left-hand side (tidyverse)
    #using glue {{}} with := assignment operator
    #https://stackoverflow.com/questions/26003574/use-dynamic-name-for-new-column-variable-in-dplyr
     mutate({{this_col_name}} := (mod_wk_dt_yr == this_yr))
  
}

# record years (or none) for text in report
if (length(yrs_mir_imputed) == 0) {
  yrs_mir_imputed_text <- "none"
} else {
  yrs_mir_imputed_text <- stringr::str_flatten(yrs_mir_imputed, 
                                               collapse = ", ")
}

# Now, add our final MIRsummarystat to the combined data set
if (mosquito_model_clean %in% mosq_nonstrat_models){
  
  data_combined_prep <- data_combined_prep %>% 
    #MIRsummarystat same for every week and district within a year
    dplyr::left_join(prep_mir_filler, by = "mod_wk_dt_yr")
  
} else if (mosquito_model_clean %in% mosq_strat_models){
    
  data_combined_prep <- data_combined_prep %>% 
      #join with data_strata to get strata
    dplyr::left_join(data_strata %>% 
                       select(arbo_ID, strata),
                     by = "arbo_ID") %>% 
    #MIRsummarystat same for every week and district within a year
    dplyr::left_join(prep_mir_filler, by = c("mod_wk_dt_yr", "strata"))

}





## Environmental data

# <<>><<>><<>>







# select which variables we're going to use
weather <- data.frame(weather)
weather$var1 <- weather[,which(colnames(weather) == var1name)]
weather$var2 <- weather[,which(colnames(weather) == var2name)]

weather$anom_var1 <- weather[,which(colnames(weather) == paste("anom_", var1name, sep=""))]
weather$anom_var2 <- weather[,which(colnames(weather) == paste("anom_", var2name, sep=""))]

weather <- weather[c("district", "date", "var1", "var2", "anom_var1", "anom_var2")]
weather$district <- simplifynames(weather$district)
# fill in missing and future climatological data
totalweather <- expand.grid(district=unique(weather$district),
                            date=seq(from=as.Date(paste(minhumandesiredyear-1, "-01-01", sep=""),
                                                  "%Y-%m-%d"),
                                     to=  as.Date(paste(maxhumandesiredyear, "-12-31", sep=""),
                                                  "%Y-%m-%d"),
                                     by=1))
weather <- merge(x=totalweather, y=weather,
                 by.x=c("district","date"),
                 by.y=c("district","date"),
                 all.x=TRUE)
weather$doy <- as.numeric(format(weather$date, "%j"))

weather <- group_by(weather, district, doy)
districtdoymean <- dplyr::summarize(weather,
                                    meanvar1=mean(var1, na.rm=TRUE),
                                    meanvar2=mean(var2, na.rm=TRUE),
                                    .groups="keep")
weather <- ungroup(weather)                           

weather <- left_join(weather, districtdoymean,
                     by=c("district","doy"))

# replace missing with reasonable values
weather$var1[is.na(weather$var1)] <- weather$meanvar1[is.na(weather$var1)]
weather$var2[is.na(weather$var2)] <- weather$meanvar2[is.na(weather$var2)]
weather$anom_var1[is.na(weather$anom_var1)] <- 0
weather$anom_var2[is.na(weather$anom_var2)] <- 0

# garbage collection
rm(districtdoymean)
weather$meanvar1 <- NULL
weather$meanvar2 <- NULL
gc()

# we need the districts to be in lower case to merge with the gridMET data
fullcasemat$district <- simplifynames(fullcasemat$district)

datalagger <- expand.grid(unique(fullcasemat$district),
                          unique(fullcasemat$weekstartdate),
                          seq(from=0, to=laglen-1, by=1))
names(datalagger) <- c("district","date","lag")
datalagger$laggeddate <- datalagger$date-datalagger$lag

datalagger <- left_join(datalagger, weather,
                        by=c("district"="district",
                             "laggeddate"="date"))

# garbage collection
rm(weather)
gc()

# pivot
mean1data <- dcast(datalagger, district + date ~ lag, value.var="var1")
names(mean1data) <- paste("var1_",names(mean1data),sep="")
mean2data <- dcast(datalagger, district + date ~ lag, value.var="var2")
names(mean2data) <- paste("var2_",names(mean2data),sep="")

anom_mean1data <- dcast(datalagger, district + date ~ lag, value.var="anom_var1")
names(anom_mean1data) <- paste("anom_var1_",names(anom_mean1data),sep="")
anom_mean2data <- dcast(datalagger, district + date ~ lag, value.var="anom_var2")
names(anom_mean2data) <- paste("anom_var2_",names(anom_mean2data),sep="")

# garbage collection
rm(datalagger)
gc()

# and put all this lagged info back into the total case matrix
fullcasemat <- merge(x=fullcasemat, y=mean1data,
                     by.x=c("district", "weekstartdate"),
                     by.y=c("var1_district","var1_date"),
                     all.x=TRUE)
fullcasemat <- merge(x=fullcasemat, y=mean2data,
                     by.x=c("district", "weekstartdate"),
                     by.y=c("var2_district","var2_date"),
                     all.x=TRUE)

# turn these into matrices
fullcasemat$var1 <- as.matrix(fullcasemat[,grep(x=colnames(fullcasemat),
                                                pattern="var1_",
                                                fixed=TRUE)])
fullcasemat$var2 <- as.matrix(fullcasemat[,grep(x=colnames(fullcasemat),
                                                pattern="var2_",
                                                fixed=TRUE)])
fullcasemat <- merge(x=fullcasemat, y=anom_mean1data,
                     by.x=c("district", "weekstartdate"),
                     by.y=c("anom_var1_district","anom_var1_date"),
                     all.x=TRUE)
fullcasemat <- merge(x=fullcasemat, y=anom_mean2data,
                     by.x=c("district", "weekstartdate"),
                     by.y=c("anom_var2_district","anom_var2_date"),
                     all.x=TRUE)

# turn these into matrices too
fullcasemat$anom_var1 <- as.matrix(fullcasemat[,grep(x=colnames(fullcasemat),
                                                     pattern="anom_var1_",
                                                     fixed=TRUE)])
fullcasemat$anom_var2 <- as.matrix(fullcasemat[,grep(x=colnames(fullcasemat),
                                                     pattern="anom_var2_",
                                                     fixed=TRUE)])

fullcasemat[,grep(x=colnames(fullcasemat), pattern="var1_", fixed=TRUE)] <- NULL
fullcasemat[,grep(x=colnames(fullcasemat), pattern="var2_", fixed=TRUE)] <- NULL
fullcasemat[,grep(x=colnames(fullcasemat), pattern="anom_var1_", fixed=TRUE)] <- NULL
fullcasemat[,grep(x=colnames(fullcasemat), pattern="anom_var2_", fixed=TRUE)] <- NULL

# create lag matrix
fullcasemat$lag <- fullcasemat$var1
for (curcol in 1:dim(fullcasemat$lag)[2]) {
  
  fullcasemat$lag[,curcol] <- (curcol-1)
  
}

# create terms for seasonally-varying distributed lag
fullcasemat$doymat <- fullcasemat$lag
fullcasemat$doymat[,] <- fullcasemat$doy

fullcasemat$year <- as.numeric(format(fullcasemat$weekstartdate, "%Y"))

# do we have strata?
if (mosqsummarystyle %in% c("stratifiedMIGR", "stratifiedMII")) {
  
  fullcasemat <- left_join(fullcasemat, strata, by="district")
  fullcasemat$stratumyear <- paste(fullcasemat$strata,
                                   fullcasemat$year,
                                   sep="-")
  
}

# garbage collection
rm(mean1data)
rm(mean2data)
gc()


```


```{r forecast_modeling, echo=FALSE}

#<<>><<>><<>>
# rem GAM dev switch
# rem get model cache code from other project


# set up list to contain models
modelplots <- list()

# make sure district is a factor before modeling
fullcasemat$district <- factor(fullcasemat$district)

# get list of models and run them
preds <- data.frame()
modelevals <- data.frame()
curmodel <- modelnames[1]
for (curmodel in modelnames) {
  
  # get the formula
  thisformula <- modelformulas[c(curmodel)]
  
  # add any exact fits
  exactfitlist <- colnames(fullcasemat)[grep(x=colnames(fullcasemat),
                                             pattern="exactfit_",
                                             fixed=TRUE)]
  if (length(exactfitlist) > 0) {
    
    exactfitlist <- paste(exactfitlist, collapse = " + ")
    thisformula <- paste(thisformula, exactfitlist, sep= " + ")
    
  }
  myform <- as.formula(thisformula)
  
  # main human risk model
  cl <- makeCluster(detectCores(logical=FALSE)-1)
  firstreg  <- bam(formula=myform,
                   family=binomial(), data=fullcasemat,
                   subset=modeled==1,
                   cluster=cl)
  stopCluster(cl)
  
  # add model plots to the list
  modelplots[[curmodel]] <- plot(firstreg, select=1)
  
  # predict on this model
  fullcasemat$pred <- predict(firstreg, newdata=fullcasemat, newlevels=TRUE, type="response")
  
  # censor predictions outside of the observed range
  fullcasemat$pred[fullcasemat$weeknum < minhumanobsweek] <- 0
  fullcasemat$pred[fullcasemat$weeknum > maxhumanobsweek] <- 0
  
  # get data frame of predictions from this model
  tempdf <- data.frame(weekstartdate = fullcasemat$weekstartdate,
                       district = fullcasemat$district,
                       anycases = fullcasemat$anycases,
                       totalcases = fullcasemat$totalcases,
                       observed = fullcasemat$observed,
                       modeled = fullcasemat$modeled,
                       pred = fullcasemat$pred,
                       model = curmodel)
  preds <- bind_rows(preds, tempdf)
  
  # evaluate this model fit
  tempdf <- fullcasemat[fullcasemat$modeled == 1,]
  tempdf <- data.frame(auc = round_any(roc(response = tempdf$anycases,
                                           predictor = tempdf$pred,
                                           na.rm=TRUE,
                                           auc=TRUE)$auc, 0.01),
                       aic = round_any(AIC(firstreg)[1], 0.01),
                       model = curmodel)
  modelevals <- bind_rows(modelevals, tempdf)
  
}


```

<!-- End of internals: data load, processing, forecast modeling, etc. -->
<!-- Everything below is report: text and figures. -->








```{r , fig.width=7, fig.height=3, echo=FALSE, include=FALSE}
```

```{r , fig.width=7, fig.height=3, echo=FALSE, include=TRUE}
```


```{r , include=TRUE, echo=FALSE, fig.width=6, fig.height=3}
```


```{r , echo=FALSE, include=TRUE, fig.width=7, fig.height=3.5}
```


```{r , echo=TRUE, warnings=FALSE, include=FALSE}
```


```{r , echo=FALSE, include=FALSE}

```

```{r , echo=FALSE}
```


```{r , fig.width=7, fig.height=4, echo=FALSE, warnings=FALSE}
```


```{r fig.width=7, fig.height=4, echo=FALSE, warnings=FALSE}
```


```{r params_pretty, include=TRUE, echo=FALSE}
#<<>>


```


```{r appendix_switch, child=if (params$create_appendix) 'ArboMAP_forecast_appendix.Rmd'}
# Conditionally add the appendix child file (to be created) depending the T/F value of create_appendix 
#https://bookdown.org/yihui/rmarkdown-cookbook/child-document.html

# <<>> DEV look at epidemia report to see what, if anything, needs set up here. 

```
