---
params:
  ## week to run forecast for
  forecast_date: "2018-08-15"
  ## state
  state_name: "South Dakota"
  state_code: "SD"
  ## predictors 
  predictor_var1: "tmeanc"
  predictor_var2: "vpd"
  ## mosquito settings
  mosquito_model: "AUC"
  mosquito_doy_start: 140
  mosquito_doy_end: 214
  ## input data file locations
  file_human: !r file.path("data_human", "simulated human case data.csv")
  file_mosquito: !r file.path("data_mosquito", "simulated mosquito tests.csv")
    #if no strata, set to ""
  file_stratification: !r file.path("data_strata", "example SD strata.csv")
    #if do not have sf object of counties saved as an RDS file, 
      #temporarily set to "create" once, and it will make the appropriate state file. Must have internet access.
    #if do not want to cache, 
      #set to "always_download" and it will download tigris shapefile each time. Must have internet acces.
  file_district_sf: !r file.path("data_spatial", "sd_counties.RDS")
  #dev, limited models <<>>
  #file_models: !r file.path("data_models", "models.txt")
  file_models: !r file.path("dev", "models_tpfx.txt")
  folder_weather: "data_weather"
  ## data range settings
  #which years of human data to use
  year_human_start: 2004
  year_human_end: 2017
  #which years of mosquito data to use
  year_mosquito_start: 2012
  year_mosquito_end: 2018
  #which years of weather data to use
  year_weather_start: 2000
  year_weather_end: 2018
  #which years to include in modeling results
  year_modeling_start: 2004
  year_modeling_end: 2018
  #which years to show as comparison years in graphs
  year_compare_vis1: 2012
  year_compare_vis2: 2017
  ## additional settings
  # length (days) of weather data to include in lags
  lag_length: 121
  # remove temporal outliers from human cases
  case_trim_alpha: 0.02
  # resampling mosquito <<>>
  resample_mosquito: FALSE
  resample_file: NA
  # developer settings
  dev_settings: !r list()

title: "ArboMAP: Arbovirus Modeling and Prediction   \nto Forecast Mosquito-Borne Disease Outbreaks"
author: "Summary of Model Outputs (v3.1) for `r params$state_name`, `r params$forecast_date`  \nDawn M. Nekorchuk, Justin K. Davis, and Michael C. Wimberly  \n(mcwimberly@ou.edu)  \nGeography and Environmental Sustainability, University of Oklahoma"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---
  
```{r knitr_setup, include=FALSE}
options(warn=-1)
```

```{r libraries, include=FALSE}

#make sure pacman is installed
if (!require("pacman")) install.packages("pacman"); library(pacman)

#load packages, install if not installed
pacman::p_load(
  #data processing. 
  dplyr, readr, tidyselect, 
  #modeling
  mgcv, splines,
  #spatial, maps and graphs
  tigris, sf, ggplot2,
  #report generation and interface
  knitr, shiny)

#Must use recent version of readr
if (packageVersion("readr") < 2.1){
  install.packages("readr")
}

```

```{r functions, include=FALSE}

# define some helpful functions
# <<>> confirm all are used

'%!in%' <- function(x,y)!('%in%'(x,y))

round_any = function(x, accuracy, f=round){f(x/accuracy) * accuracy}

simplifynames <- function(priornames=NULL) {
  
  # convert to lower case
  priornames <- tolower(priornames)
  
  # remove spaces
  priornames <- gsub(pattern=" ", replacement="", x=priornames, fixed=TRUE)
  
  # remove other offending placename modifiers
  priornames <- gsub(pattern="county", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="parish", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="par.", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="(zone)", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="lower", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="upper", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="southern", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="northern", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="saint", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="st", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern=".", replacement="", x=priornames, fixed=TRUE)
  
  # return names
  return(priornames)
  
}


```


```{r dev_parameters, include=FALSE}
#input is named list

#parameters available:
# 

  # save_model: TRUE/FALSE: Will create a list of saved model objects (using rest of input params)
  # model_cached: must be named list of model objects, named from modeling file to pattern match
  # model_cached_file: "no cached" <<>>????

  # human_data: tbl of human case data (overrides file_human)
  # mosquito_data: tbl of mosquito pool data (overrides file_mosquito)
  # stratification_data: tbl of strata (overrides file_stratification)
  # weather_data: tbl of weather data (overrides folder_weather & processing)
  # district_sf: sf object of counties/districts for state (overrides file_district_sf)

  # models_to_run: model formulas to run (overrides file_models)

  # reg_function: "GAM" (hook for future possibility)

#Set up data objects (will use as tests for file loading in following section)
data_human <- NA
data_mosquito <- NA
data_strata <- NA
data_weather <- NA
data_sf_orig <- NA
model_formulas <- NA

#<<>> do overrides. if (length(params$dev_settings > 0). chk epidemiar dev settings

```




```{r data_load, include=FALSE}
#Loads data from file locations given in parameters
  #Note: does NOT do any data checks

if (is.na(data_human)){
  data_human <- readr::read_csv(params$file_human, 
                                show_col_types = FALSE)
}

if (is.na(data_mosquito)){
  data_mosquito <- readr::read_csv(params$file_mosquito, 
                                   show_col_types = FALSE)
}

if (is.na(data_strata)){
  #if given a strata file, which is optional
  if (!params$file_stratification == ""){
    data_strata <- readr::read_csv(params$file_stratification, 
                                   show_col_types = FALSE)
  }
}

if (is.na(data_sf_orig)){
  
  if (params$file_district_sf == "create"){
    #if user set to "create" then we will download tigris shapefile and save for future use
    
    #download tigris, internet required
    data_sf_orig <- tigris::counties(state = params$state_code, cb = TRUE)
    #save out for use next time
    #make folder if does not exist (if exists, just shows warning, suppressed)
    dir.create("data_spatial", showWarnings = FALSE)
    saveRDS(data_sf_orig, file.path("data_spatial", paste0(params$state_code, "_counties.RDS")))

  } else if (params$file_district_sf == "always_download"){
    #if "always_download" then we will download tigris shapefile each time (no save), internet required
    
    data_sf_orig <- tigris::counties(state = params$state_code, cb = TRUE)

  } else {
    #read in file from params
    
    data_sf_orig <- readRDS(params$file_district_sf)
    
  }
}#end is.na



if (is.na(model_formulas)){
  models_raw <- readr::read_csv(params$file_models, 
                                show_col_types = FALSE, quote = "\"")
  #create named list from tbl
  model_formulas <- models_raw %>% 
    dplyr::pull(2, name = 1)
  #just the model names for later use
  model_names <- names(model_formulas)
}

# Weather data
# Raw read in with prep for taking most recent value for day
# See data_weather_latest block for that processing

# Reading in of data & file modified time
#get list of csv files (NOT in subfolders)
env_csv_files_raw <- list.files(path = params$folder_weather, 
                                pattern="*.csv$",
                                full.names = TRUE, recursive = FALSE)

#keep the names of only csv files that are not empty
  #not likely relevant here, however does no harm to check
file_condition <- sapply(env_csv_files_raw, function(x) {length(readr::count_fields(x, readr::tokenizer_csv())) > 1})
env_csv_files <- env_csv_files_raw[file_condition]
  
#read in all data files, and add the time the file was last modified
data_env_raw <- env_csv_files %>% 
  lapply(function(x) {
    readr::read_csv(x, show_col_types = FALSE) %>% 
      #add last modified time
    mutate(file_time = file.info(x)$mtime)}) %>% 
  #bind list items into one dataset
  bind_rows()


```

```{r data_id_fields, echo=FALSE} 

#ID fields:
# If FIPS field is in all, will use fips (FULL 5 character version)
# Else use original district/county name matching
# Accepted field names here, there will be preferred, 
# but this gives some flexibility 
# Processing happens after read in, will create "arbo_ID" used afterwards
# Processing includes wrangling fips to match across all files
# Each list in DESCENDING order of priority
#   will only take the field that appears first
field_fips_accepted <- c("fips", "FIPS", "fips_code", "FIPS_CODE")
field_names_accepted <- c("district", "county")


#ID Functions
confirm_id_fields <- function(fld_vector){
  #Does any of the accepted fields (of a particular type)
  # exist in all 4 datasets
  
  my_count <- sum(
    any(fld_vector %in% names(data_human)),
    any(fld_vector %in% names(data_mosquito)),
    any(fld_vector %in% names(data_strata)),
    any(fld_vector %in% names(data_env_raw))
    )
  
  #true/false
  use_fld <- my_count == 4L
}

create_id_field <- function(my_tbl, fld_vector){
  #the field names in the dataset that match the accepted names
  # [[1]] takes the first
  field_to_copy <- intersect(fld_vector, names(my_tbl))[[1]]

  updated_tbl <- my_tbl %>% 
    mutate(arbo_ID = !!ggplot2::sym(field_to_copy))
}

standarize_fips <- function(my_tbl, fips_vector = field_fips_accepted){
  
  # Create new field arbo_ID which will be used for matching now on
  arbo_tbl <- create_id_field(my_tbl, fips_vector)
  
  # convert to standard 5 character (2 state + 3 county) format
  # if length 5, confirm/convert to character
  # if length 4, then full but read as number and state has leading 0
  #   convert to character, pad 0 in front
  # if length 3, confirm/convert to character, add state code
  # if length 2, then county code but read as number and county has leading 0
  #   convert to character, pad 0 in front to 3,
  #   then add state code
  
  #grab state code from shp 
  state_code <- data_sf_orig$STATEFP %>% unique()
  
  #<<>>

}

standarize_names <- function(my_tbl, names_vector = field_names_accepted){
  
  #Create new field arbo_ID which will be used for matching now on
  arbo_tbl <- create_id_field(my_tbl, names_vector)

  #Use original name simplification
  arbo_tbl <- arbo_tbl %>% 
    mutate(arbo_ID = simplifynames(arbo_ID))
}


# Set up arbo_ID 
if (confirm_id_fields(field_fips_accepted)){
  # use fips as arbo_ID

  data_human <- standarize_fips(data_human)
  data_mosquito <- standarize_fips(data_mosquito)
  data_strata <- standarize_fips(data_strata)
  data_env_raw <- standarize_fips(data_env_raw)
  
  #  data_sf_orig will be dealt with separately, as it is standard format
  #census shape: GEOID is 5 char FIPS code
  data_sf_orig <- data_sf_orig %>% 
    mutate(arbo_ID = GEOID)
  
} else if (confirm_id_fields(field_names_accepted)){
  # use county names as arbo_ID

  data_human <- standarize_names(data_human)
  data_mosquito <- standarize_names(data_mosquito)
  data_strata <- standarize_names(data_strata)
  data_env_raw <- standarize_names(data_env_raw)

  #  data_sf_orig will be dealt with separately, as it is standard format
  #census shape: NAME is county name
  data_sf_orig <- data_sf_orig %>% 
    mutate(arbo_ID = simplifynames(NAME))

}#end arbo_ID setup



```



```{r data_weather_latest, echo=FALSE} 

# As we will likely have data from the same day in multiple files,
# we want to take only the LATEST value
# (post ID set up section to remove extraneous fields before processing)

data_env <- data_env_raw %>% 
  #trim to just needed data
  select("arbo_ID", "doy", "year", "file_time", 
         params$predictor_var1, params$predictor_var2) %>% 
  #create date field from year and doy
  mutate(date_obs = as.Date(paste(year, doy, sep = "-"),
                            "%Y-%j")) %>% 
  # Group by county and date, 
  group_by(arbo_ID, date_obs) %>% 
  # sort by modified time DESC, 
  arrange(desc(file_time)) %>% 
  # slice(1) to get first/latest (or only if not duplicated)
  slice(1) %>% 
  #drop file time
  dplyr::select(-file_time) %>%
  #ungroup to finish
  dplyr::ungroup()

#Note: This does not check for missing days

```




```{r data_dates, echo=FALSE} 

# Dates in data

#try reading as previously set format
# if not, use as.Date() fairly intelligent guessing
# "date_obs" will become the date field!!
#<<>>

#wnv$col_date <- as.Date(wnv$col_date, "%m/%d/%Y")


## Filter data by year parameters
#<<>>
  # #which years of human data to use
  # year_human_start: 2004
  # year_human_end: 2017
  # #which years of mosquito data to use
  # year_mosquito_start: 2012
  # year_mosquito_end: 2018
  # #which years of weather data to use
  # year_weather_start: 2000
  # year_weather_end: 2018
  # #which years to include in modeling results
  # year_modeling_start: 2004
  # year_modeling_end: 2018



## Dates for forecasts and functions

#week of forecast, given by user
week_user <- as.Date(params$forecast_date, "%Y-%m-%d")

#<<>> DEV keeping old date processing for the moment until conversion to epiweeks

  # makes sure we round to the previous Sunday, so that this week is included
  TBC_weekinquestionSun <- week_user - (as.numeric(strftime(week_user, '%u')) %% 7)
  TBC_weekinquestionSat <- TBC_weekinquestionSun + 6
  TBC_weekinquestionSunstr <- strftime(TBC_weekinquestionSun, '%A')
  TBC_weekinquestionSatstr <- strftime(TBC_weekinquestionSat, '%A')
  
  # figure out which is the last Sunday in the max desired year
  TBC_maxhumandesireddate <- as.Date(paste(params$year_modeling_end, "-12-31", sep=""))
  TBC_maxhumandesireddate <- TBC_maxhumandesireddate - (as.numeric(strftime(TBC_maxhumandesireddate, '%u')) %% 7)
  TBC_maxhumandesireddatestr <- strftime(TBC_maxhumandesireddate, '%A')
  
  # figure out which is the first Sunday in the min desired human year
  TBC_minhumandesireddate <- as.Date(paste(params$year_modeling_start, "-01-01", sep=""))
  TBC_minhumandesireddate <- TBC_minhumandesireddate - (as.numeric(strftime(TBC_minhumandesireddate, '%u')) %% 7)
  TBC_minhumandesireddatestr <- strftime(TBC_minhumandesireddate, '%A')



  


  

```



```{r data_human_process, echo=FALSE} 


```

```{r data_weather_process, echo=FALSE} 


# be certain district is a factor before modeling
#weather$district <- factor(weather$district)


```

```{r data_mosquito_process, echo=FALSE} 


```

```{r mosquito_infection_model, echo=FALSE} 


```









```{r weatherplots, fig.width=7, fig.height=5, echo=FALSE}
```


```{r mosquitodataread, echo=FALSE} 

```


```{r mosquitodataprocess, echo=FALSE}
```


```{r mosquitodataprocess2, fig.width=7, fig.height=3, echo=FALSE, include=FALSE}
```

```{r fig.width=7, fig.height=3, echo=FALSE, include=TRUE}
```

```{r humandata, include=FALSE, echo=TRUE}

```


```{r mosqbymonth, include=TRUE, echo=FALSE, fig.width=6, fig.height=3}
```


```{r humandatasummary, echo=FALSE, include=TRUE, fig.width=7, fig.height=3.5}
```


```{r humandata2, echo=TRUE, warnings=FALSE, include=FALSE}
```


```{r humanreg, echo=FALSE, include=FALSE}

```

```{r humanregplot, echo=FALSE}
```


```{r fig.width=7, fig.height=4, echo=FALSE, warnings=FALSE}
```


```{r fig.width=7, fig.height=4, echo=FALSE, warnings=FALSE}
```


```{r, echo=FALSE, include=TRUE}
```


```{r curyearplot, include=TRUE, echo=FALSE, fig.width=7, fig.height=3.5}
```


```{r echo=FALSE, width=7, height=8}
```


```{r shapefile, include=TRUE, echo=FALSE, warnings=FALSE}
```



```{r miscscaves, include=FALSE, echo=FALSE}
```


```{r riskcalcs, include=TRUE, echo=FALSE, fig.width=7, fig.height=3.5}

```


```{r modelplots, include=TRUE, echo=FALSE, fig.width=7, fig.height=4}
```


```{r diagnostics, include=FALSE, echo=FALSE}
```


```{r printparams, include=TRUE, echo=FALSE}
```
