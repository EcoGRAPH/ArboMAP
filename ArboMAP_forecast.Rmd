---
params:
  ## week to run forecast for
  forecast_date: "2018-08-15"
  ## state
  state_name: "South Dakota"
  state_code: "SD"
  ## predictors 
  predictor_var1: "tmeanc"
  predictor_var2: "vpd"
  ## mosquito settings
  mosquito_model: "AUC" # "simpleratio", "AUC", "MIGR", "MII", "stratifiedMIGR", "stratifiedMII"
  mosquito_doy_start: 140
  mosquito_doy_end: 214
  ## input data file locations
  file_human: !r file.path("data_human", "simulated_human_data.csv")
  file_mosquito: !r file.path("data_mosquito", "simulated_mosquito_data.csv")
    #if no strata, set to ""
  file_strata: !r file.path("data_strata", "example_strata_SD.csv")
    #if do not have sf object of counties saved as an RDS file, 
      #temporarily set to "create" once, and it will make the appropriate state file. Must have internet access.
    #if do not want to cache, 
      #set to "always_download" and it will download tigris shapefile each time. Must have internet access.
  file_district_sf: !r file.path("data_spatial", "sd_counties.RDS")
  #dev, limited models <<>>
  #file_models: !r file.path("data_models", "models.txt")
  file_models: !r file.path("dev", "models_tpfx.txt")
  folder_weather: "data_weather"
  ## data range settings
  #which years of human data to use
  year_human_start: 2004
  year_human_end: 2017
  #which years of mosquito data to use
  year_mosquito_start: 2012
  year_mosquito_end: 2018
  #which years of weather data to use
  year_weather_start: 2000
  year_weather_end: 2018
  #which years to include in modeling results
  year_modeling_start: 2004
  year_modeling_end: 2018
  #which years to show as comparison years in graphs
  year_compare_vis1: 2012
  year_compare_vis2: 2017
  ## additional settings
  # create appendix at end with more details and graphs 
  create_appendix: TRUE
  # length (days) of weather data to include in lags
  lag_length: 121
  # remove temporal outliers from human cases
  case_trim_alpha: 0.02
  # developer settings
  dev_settings: !r list()

title: "ArboMAP: Arbovirus Modeling and Prediction   \nto Forecast Mosquito-Borne Disease Outbreaks"
author: "Summary of Model Outputs (v3.1) for `r params$state_name`, `r params$forecast_date`  \nDawn M. Nekorchuk, Justin K. Davis, and Michael C. Wimberly  \n(mcwimberly@ou.edu)  \nGeography and Environmental Sustainability, University of Oklahoma"
date: "Updated `r format(Sys.time(), '%B %d, %Y')`"
output: pdf_document
---

```{r knitr_setup, include=FALSE}
options(warn=-1)
```

```{r libraries, include=FALSE}

#make sure pacman is installed
if (!require("pacman")) install.packages("pacman"); library(pacman)

#load packages, install if not installed
pacman::p_load(
  #data processing, tidyverse related 
  dplyr, readr, tidyselect, rlang, tidyr, 
  tibble, stringr, glue, lubridate,
  #add'l data processing
  zoo, 
  #modeling & evaluation
  mgcv, splines, parallel, pROC,
  #spatial, maps and graphs
  tigris, sf, ggplot2, gridExtra, viridis,
  #report generation and interface
  knitr, shiny)

#Must use recent version of readr
if (packageVersion("readr") < 2.1){
  install.packages("readr")
}

#Single use functions are found in their section 
# for convenience when reviewing/editing code
# Most are found in the 'data_id_fields' section

```

<!-- Start of internals: data load, processing, forecast modeling, etc. -->
<!-- Code blocks: -->
<!--  dev_parameters: check for overrides from named list of dev parameters. Only really accessible if rmd called from script. -->
<!--  data_load: load all data - human, mosquito, environmenta, spatial, models. No data checks or processing here. -->
<!--  data_id_fields: add a standardized arbo_ID field based on names or fips code, depending on fields in datasets -->
<!--  data_human_processing_dx: process the human data, create diagnostics/stats -->
<!--  data_mosq_processing_dx: process the mosquito data, create diagnostics/stats -->
<!--  mosquito_resampling: optional section for resampling of mosquito data. NOTE: [DEV] Likely no longer functional from [V3] -->
<!--  mosquito_infection_model: run selected mosquito model to generate mir_stat used in modeling -->
<!--  data_env_latest_history: processes the environment data - 1) gets latest updated value for a day, 2) creates historical doy means -->
<!--  data_env_anomalization: calculates anomalies of environmental data via gam -->
<!--  data_combine: combine all data and format for passing off for regression -->
<!--  forecast_modeling: create the forecast models -->
<!-- Note on code comment flags: [V3]: this is what was done in version 3; [DEV] developer note; [DMN] note from Dawn Nekorchuk; DEV: active development -->

```{r dev_parameters, echo=FALSE, include=FALSE}
#input is named list

#parameters available:

##DATA
# human_data: tbl of human case data (overrides file_human)
# mosquito_data: tbl of mosquito pool data (overrides file_mosquito)
# stratification_data: tbl of strata (overrides file_stratification)
# weather_data: tbl of weather data (overrides folder_weather & processing)
# district_sf: sf object of counties/districts for state (overrides file_district_sf)
# models_to_run: model formulas to run (overrides file_models)

##CACHED MODELS
# save_models: TRUE/FALSE: Will create a list of saved model objects (using rest of input params)
# models_cached: must be named list of model objects, named from modeling file to pattern match (output of save_models)

##MOSQUITO
# resampling mosquito data before mosquito model
# NOTE: resampling code is mostly left as v3, and is likely inoperable 
#   unless field names are different in mosquito file that needs resampling.
#resample_mosquito: TRUE/FALSE
#resample_file: path and file

##REGRESSION
# reg_function: "GAM" (hook for future possibility)

##SAVING (for extra research files)
#out_folder: "."
#out_name_base: "ArboMAP_forecast_"
#dev_write_output: write out a bunch of files into dev folder for testing and checking

##EVALUATION
#model_evaluation: TRUE/FALSE output model evaluation statistics

#NOTE: If not using dev parameter, do NOT include in list or instead set to NULL
#     The test checks if the value is NULL. Not NA or "" or things like that. 
# dev_settings itself is set to an empty list in default params above

if (length(params$dev_settings) > 1){
  
  ##DATA
  if (!is.null(params$dev_settings$data_human)){
    #override data
    data_human <- params$dev_settings$data_human
  } else {
    #load from file, data_load code block
    data_human <- NULL
  }
  if (!is.null(params$dev_settings$data_mosquito)){
    #override data
    data_mosquito <- params$dev_settings$data_mosquito
  } else {
    #load from file, data_load code block
    data_mosquito <- NULL
  }
  if (!is.null(params$dev_settings$data_strata)){
    #override data
    data_strata <- params$dev_settings$data_strata
  } else {
    #load from file, data_load code block
    data_strata <- NULL
  }
  if (!is.null(params$dev_settings$data_weather)){
    #override data
    data_weather <- params$dev_settings$data_weather
  } else {
    #load from file, data_load code block
    data_weather <- NULL
  }
  if (!is.null(params$dev_settings$data_sf_orig)){
    #override data
    data_sf_orig <- params$dev_settings$data_sf_orig
  } else {
    #load from file, data_load code block
    data_sf_orig <- NULL
  }
  if (!is.null(params$dev_settings$model_formulas)){
    #override data
    model_formulas <- params$dev_settings$model_formulas
  } else {
    #load from file, data_load code block
    model_formulas <- NULL
  }
  
  ##CACHED MODELS
    if (!is.null(params$dev_settings$save_models)){
    #use param given
    save_models <- params$dev_settings$save_models
  } else {
    #default is FALSE
    save_models <- FALSE
  }
  #models_cached testing and possible use is done inside of forecast regression section
    if (!is.null(params$dev_settings$models_cached)){
    #use param given
    save_models <- params$dev_settings$models_cached
  } else {
    models_cached <- NULL
  }


  ##MOSQUITO
    if (!is.null(params$dev_settings$resample_mosquito)){
    #use param given
    resample_mosquito <- params$dev_settings$resample_mosquito
  } else {
    #default is FALSE
    resample_mosquito <- FALSE
  }
  #resample_file: Up to the dev user to provide path&file correctly

  ##REGRESSION
    if (!is.null(params$dev_settings$reg_function)){
    #use param given
    reg_function <- params$dev_settings$reg_function
  } else {
    #default is the current and only way via mgcv bam
    #this is being setup so that more may be added easily in the future [DMN]
    reg_function <- "GAM"
  }

  ##SAVING (for extra research files)
    if (!is.null(params$dev_settings$out_folder)){
    #use param given
    out_folder <- params$dev_settings$out_folder
  } else {
    out_folder <- "."
  }
    if (!is.null(params$dev_settings$out_name_base)){
    #use param given
    out_name_base <- params$dev_settings$out_name_base
  } else {
    #default 
    #out_name_base <- "ArboMAP_forecast_"
    # <<>> DEV testing
    out_name_base <- paste0(params$mosquito_model, "_", Sys.time() %>% format("%Y%m%d%H%M"))
  }
    if (!is.null(params$dev_settings$dev_write_output)){
    #use param given
    dev_write_output <- params$dev_settings$dev_write_output
  } else {
    dev_write_output <- FALSE
  }


  ##EVALUATION
    if (!is.null(params$dev_settings$model_evaluation)){
    #use param given
    model_evaluation <- params$dev_settings$model_evaluation
  } else {
    #default is to not save extra files, 
    # only currently used in research comparing many models
    model_evaluation <- FALSE
  }

  ## OTHER
  #[V3] [DEV] imputemissingdistricts was hard coded as FALSE in v3 production code
  # updated names and code, and added dev option for renamed impute_human_missing_districts, default = FALSE
  # DEV This looks like it samples existing districts as data for the missing districts.
  #     I can't think of a time where this would be wanted. May be able to delete in future. [DMN]
    if (!is.null(params$dev_settings$impute_human_missing_districts)){
    #use param given
    impute_human_missing_districts <- params$dev_settings$impute_human_missing_districts
  } else {
    #should stay FALSE unless you are very sure
    impute_human_missing_districts <- FALSE
  }

  
  
} # end if (length(params$dev_settings >= 1)

```

```{r data_load, echo=FALSE, include=FALSE}
#Loads data from file locations given in parameters
#Note: does NOT do any data checks

if (is.null(data_human)){
  data_human <- readr::read_csv(params$file_human, 
                                show_col_types = FALSE)
}

if (is.null(data_mosquito)){
  data_mosquito <- readr::read_csv(params$file_mosquito, 
                                   show_col_types = FALSE)
}

if (is.null(data_strata)){
  #if given a strata file, which is optional
  if (!params$file_strata == ""){
    data_strata <- readr::read_csv(params$file_strata, 
                                   show_col_types = FALSE)
  }
}

if (is.null(data_sf_orig)){
  
  if (params$file_district_sf == "create"){
    #if user set to "create" then we will download tigris shapefile and save for future use
    
    #download tigris, internet required
    data_sf_orig <- tigris::counties(state = params$state_code, cb = TRUE)
    #save out for use next time
    #make folder if does not exist (if exists, just shows warning, suppressed)
    dir.create("data_spatial", showWarnings = FALSE)
    saveRDS(data_sf_orig, file.path("data_spatial", paste0(params$state_code, "_counties.RDS")))
    
  } else if (params$file_district_sf == "always_download"){
    #if "always_download" then we will download tigris shapefile each time (no save), internet required
    
    data_sf_orig <- tigris::counties(state = params$state_code, cb = TRUE)
    
  } else {
    #read in file from params
    
    data_sf_orig <- readRDS(params$file_district_sf)
    
  }
}#end is.null



if (is.null(model_formulas)){
  models_raw <- readr::read_csv(params$file_models, 
                                col_names = FALSE, show_col_types = FALSE, 
                                quote = "\"")
  #create named list from tbl
  model_formulas <- models_raw %>% 
    dplyr::pull(2, name = 1)
  #just the model names for later use
  model_names <- names(model_formulas)
}

# Weather data
# Raw read in with prep for taking most recent value for day
# See data_weather_latest block for that processing

# Reading in of data & file modified time
#get list of csv files (NOT in subfolders)
env_csv_files_raw <- list.files(path = params$folder_weather, 
                                pattern="*.csv$",
                                full.names = TRUE, recursive = FALSE)

#keep the names of only csv files that are not empty
#not likely relevant here, however does no harm to check
file_condition <- sapply(env_csv_files_raw, function(x) {length(readr::count_fields(x, readr::tokenizer_csv())) > 1})
env_csv_files <- env_csv_files_raw[file_condition]

#read in all data files, and add the time the file was last modified
data_env_raw <- env_csv_files %>% 
  lapply(function(x) {
    readr::read_csv(x, show_col_types = FALSE) %>% 
      #add last modified time
      dplyr::mutate(file_time = file.info(x)$mtime)}) %>% 
  #bind list items into one dataset
  dplyr::bind_rows()


#Record row count of data as read in
dx_mosq_nrow_0load <- nrow(data_mosquito)
dx_human_nrow_0load <- nrow(data_human)
dx_env_nrow_0load <- nrow(data_env_raw) #note PRE de-duplication


```

```{r data_id_fields, echo=FALSE, include=FALSE} 

#ID fields:
# If FIPS field is in all, will use fips (FULL 5 character version)
# Else use original district/county name matching
# Accepted field names here, there will be preferred, 
# but this gives some flexibility 
# Processing happens after read in, will create "arbo_ID" used afterwards
# Processing includes wrangling fips to match across all files
# Each list in DESCENDING order of priority
#   will only take the field that appears first
field_fips_accepted <- c("fips", "FIPS", "fips_code", "FIPS_CODE")
field_names_accepted <- c("district", "county")


#ID Functions
confirm_id_fields <- function(fld_vector){
  #Does any of the accepted fields (of a particular type)
  # exist in all 3 or 4 datasets
  # strata is OPTIONAL
  
  if (!is.null(data_strata)){
    #4 datasets
    
    my_count <- sum(
      any(fld_vector %in% names(data_human)),
      any(fld_vector %in% names(data_mosquito)),
      any(fld_vector %in% names(data_env_raw)),
      any(fld_vector %in% names(data_strata))
    )
    
    #true/false
    use_fld <- my_count == 4L
    
  } else {
    #3 datasets
    
    my_count <- sum(
      any(fld_vector %in% names(data_human)),
      any(fld_vector %in% names(data_mosquito)),
      any(fld_vector %in% names(data_env_raw))
    )
    
    #true/false
    use_fld <- my_count == 3L
    
  }
  
  return(use_fld)
  
} 

create_id_field <- function(my_tbl, fld_vector){
  #the field names in the dataset that match the accepted names
  # [[1]] takes the first
  field_to_copy <- intersect(fld_vector, names(my_tbl))[[1]]
  
  updated_tbl <- my_tbl %>% 
    #slight weirdness with dynamic field name in tidyverse
    #using glue {{}}
    #dplyr::mutate(arbo_ID = {{field_to_copy}})
    #glue solution stopped working 2022-03-14? unknown reasons but only RHS
    #using pseudo-base solution instead
    dplyr::mutate(arbo_ID = .data[[field_to_copy]])
}

standarize_fips <- function(my_tbl, fips_vector = field_fips_accepted){
  
  # Create new field arbo_ID which will be used for matching now on
  arbo_tbl <- create_id_field(my_tbl, fips_vector)
  
  # convert to standard 5 character (2 state + 3 county) format
  # if length 5, confirm/convert to character
  # if length 4, then full but read as number and state has leading 0
  #   convert to character, pad 0 in front
  # if length 3, confirm/convert to character, add state code
  # if length 2, then county code but read as number and county has leading 0
  #   convert to character, pad 0 in front to 3,
  #   then add state code
  
  #grab state code from shp 
  state_code <- data_sf_orig$STATEFP %>% unique()
  
  #<<>> DEV
  
}

simplifynames <- function(priornames=NULL) {
  
  #ORIGINAL name matching 
  
  # convert to lower case
  priornames <- tolower(priornames)
  
  # remove spaces
  priornames <- gsub(pattern=" ", replacement="", x=priornames, fixed=TRUE)
  
  # remove other offending placename modifiers
  priornames <- gsub(pattern="county", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="parish", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="par.", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="(zone)", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="lower", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="upper", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="southern", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="northern", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="saint", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern="st", replacement="", x=priornames, fixed=TRUE)
  priornames <- gsub(pattern=".", replacement="", x=priornames, fixed=TRUE)
  
  # return names
  return(priornames)
  
}


standarize_names <- function(my_tbl, names_vector = field_names_accepted){
  
  #Create new field arbo_ID which will be used for matching now on
  arbo_tbl <- create_id_field(my_tbl, names_vector)
  
  #Use original name simplification
  arbo_tbl <- arbo_tbl %>% 
    dplyr::mutate(arbo_ID = simplifynames(arbo_ID))
}



# Set up arbo_ID 
if (confirm_id_fields(field_fips_accepted)){
  # use fips as arbo_ID
  
  data_human <- standarize_fips(data_human)
  data_mosquito <- standarize_fips(data_mosquito)
  data_env_raw <- standarize_fips(data_env_raw)
  
  if (!is.null(data_strata)){
    data_strata <- standarize_fips(data_strata)
  }
  
  #  data_sf_orig will be dealt with separately, as it is standard format
  #census shape: GEOID is 5 char FIPS code
  data_sf_orig <- data_sf_orig %>% 
    dplyr::mutate(arbo_ID = GEOID)
  
} else if (confirm_id_fields(field_names_accepted)){
  # use county names as arbo_ID
  
  data_human <- standarize_names(data_human)
  data_mosquito <- standarize_names(data_mosquito)
  data_env_raw <- standarize_names(data_env_raw)
  
  if (!is.null(data_strata)){
    data_strata <- standarize_names(data_strata)
  }
  
  
  #  data_sf_orig will be dealt with separately, as it is standard format
  #census shape: NAME is county name
  data_sf_orig <- data_sf_orig %>% 
    dplyr::mutate(arbo_ID = simplifynames(NAME))
  
}#end arbo_ID setup

#Create a crosswalk from arbo_ID to 'pretty' names (data_sf_orig$NAME)
id_crosswalk <- data_sf_orig %>% 
  sf::st_drop_geometry() %>% 
  dplyr::select(NAME, arbo_ID)


```

```{r data_dates, echo=FALSE, include=FALSE} 

# Dates in data
#   "date_obs" will become the standard date field name of the observed
#   "date_epi" will become the standard date for modelling weeks (END date of epiweek)
# Filtering on years given in parameters
# Setting up week-related variables, in CDC epiweeks

##Functions
# epiwday borrowed from epidemiar, originally written by Chris Merkord, defaults changed here
epiwday <- function(x, system = "CDC") {
  week_type <- match.arg(system, c("ISO", "CDC"))
  if (week_type == "ISO") {
    as.integer(lubridate::wday(x - 1))
  } else if (week_type == "CDC") {
    as.integer(lubridate::wday(x))
  }
}
# make_date_yw borrowed from epidemiar, originally written by Chris Merkord, defaults changed here
make_date_yw <- function(year = 1970L, week = 1L, weekday = 7L, system = "CDC") {
  #year: epidemiological year
  #week: epidemiological week number (1--53).
  #weekday: epidemiological weekday number (1--7). Day 1 is a Monday in
  #   the ISO-8601 WHO system and a Sunday in the CDC system. DEFAULT is LAST day of week (7)
  #system: String indicating the standard (WHO ISO-8601 or CDC epiweeks) ["ISO" or "CDC"]. 
  week_type <- match.arg(system, c("ISO", "CDC"))
  lengths <- vapply(list(year, week, weekday), length, 1, USE.NAMES = FALSE)
  if (min(lengths) == 0L) as.Date(integer(), lubridate::origin)
  # recycle arguments
  N <- max(lengths)
  y <- rep_len(as.integer(year), N)
  w <- rep_len(as.integer(week), N)
  d <- rep_len(as.integer(weekday), N)

  out <-
    ifelse(
      is.na(y) | is.na(w) | is.na(d), NA,
      {
        jan1 <- lubridate::make_date(y, 1, 1)
        wday <- epiwday(jan1, week_type)
        to_add <- ifelse(wday <= 4, 1, 8) - wday
        wk1 <- jan1 + to_add
        day1 <- wk1 + (w - 1) * 7
        day1 + d - 1
      }
    )
  as.Date(out, lubridate::origin)
}
# find last epiweek in a year: will be either 52 or 53
get_last_epiweek <- function(year){
  #first pass, get epiweek of Dec 31 of the year
  dec31 <- as.Date(paste0(year, "-12-31"), "%Y-%m-%d")
  dec31_epiweek <- lubridate::epiweek(dec31)
  # Dec 31 might fall into 1st week of following year
  if (dec31_epiweek == 1){
    #if so, then get epiweek of the week prior instead
    prev_epiweek <- lubridate::epiweek(dec31 - lubridate::weeks(1))
    return(prev_epiweek)
  } else {return(dec31_epiweek)}
}

# Note: tryCatch not helpful b/c of as.Date return values 
# > as.Date("2019-6-4")
# [1] "2019-06-04"
# > as.Date("2019-6-4", "%m/%d/%Y")
# [1] NA
# > as.Date("6/4/2019")
# [1] "0006-04-20"
# also note that tryCatch doesn't work inside mutate plain
# need rowwise or other solution to use that
# So below solution is not quite as robust as it could be, 
#  but it should be pretty good

data_human <- data_human %>% 
  #try old specified format first
  #gives NA when given dates like "2019-6-4"
  dplyr::mutate(date_obs = as.Date(date, format = "%m/%d/%Y"),
                #test for NA and let as.Date guess this time
                #MUST use ifelse, not if_else b/c that evaluates all
                # and as.Date will throw error if given the old format without pattern
                # however ifelse strips date format, so must cast it afterwards
                # note: zoo package used for default origin for as.Date()
                date_obs = as.Date(ifelse(is.na(date_obs), 
                                          as.Date(date), 
                                          date_obs)))

data_mosquito <- data_mosquito %>% 
  #try old specified format first
  #gives NA when given dates like "2019-6-4"
  dplyr::mutate(date_obs = as.Date(col_date, format = "%m/%d/%Y"),
                #test for NA and let as.Date guess this time
                #MUST use ifelse, not if_else b/c that evaluates all
                # and as.Date will throw error if given the old format without pattern
                # however ifelse strips date format, so must cast it afterwards
                # note: zoo package used for default origin for as.Date()
                date_obs = as.Date(ifelse(is.na(date_obs), 
                                          as.Date(col_date), 
                                          date_obs)))


## Filter data by year parameters
# More data may be present in the files than what we want to use 
#   e.g. incomplete year data
# Adds useful date parts as fields for use here and later
# Note: Epi year is used for filtering for consistency with using epiweeks

data_human <- data_human %>% 
  #year field for filtering
  dplyr::mutate(#epi year
                year_epi = lubridate::epiyear(date_obs),
                #epiweek
                week_epi = lubridate::epiweek(date_obs),
                date_epi = make_date_yw(year = year_epi, week = week_epi, weekday = 7),
                #also doy
                doy = lubridate::yday(date_obs)) %>% 
  #filter year range from parameter input
  dplyr::filter(year_epi >= params$year_human_start & 
                  year_epi <= params$year_human_end)


data_mosquito <- data_mosquito %>% 
  #date fields for filtering (here and later in mosq modelling)
  dplyr::mutate(year_cal = lubridate::year(date_obs),
                #epi year and week
                year_epi = lubridate::epiyear(date_obs),
                week_epi = lubridate::epiweek(date_obs),
                date_epi = make_date_yw(year = year_epi, week = week_epi, weekday = 7),
                #also doy, used later in mosq modelling
                doy = lubridate::yday(date_obs)) %>% 
  #filter year range from parameter input
  dplyr::filter(year_epi >= params$year_mosquito_start & 
                  year_epi <= params$year_mosquito_end) 
#Note: doy filtering happens in code block data_mosq_processing_dx below


data_env_raw <- data_env_raw %>% 
  #already has year field from GEE <<>> DEV document in user's guide data requirements
  #filter year range from parameter input
  # for consistency, this ALSO will be epi year
  dplyr::mutate(date_obs = as.Date(paste(year, doy, sep = "-"),
                                   "%Y-%j"),
                #epi year and week
                year_epi = lubridate::epiyear(date_obs),
                week_epi = lubridate::epiweek(date_obs),
                date_epi = make_date_yw(year = year_epi, week = week_epi, weekday = 7)) %>%  
  dplyr::filter(year_epi >= params$year_weather_start & 
                  year_epi <= params$year_weather_end) 


## Dates for forecasts and functions
# Now using CDC epiweeks (MMWR week) 
# The date associated with the epiweek will be the LAST day of the epiweek

#Forecast week
#week of forecast, given by user
date_request <- as.Date(params$forecast_date, "%Y-%m-%d")
#epi week of requested date
epiweek_request <- lubridate::epiweek(date_request)
#epi year of requested date
epiyear_request <- lubridate::epiyear(date_request)
#ENDING date of the requested epi week
date_epiweek_request <- make_date_yw(year = epiyear_request, 
                                     week = epiweek_request, 
                                     weekday = 7)
#day of year of the ending date of the requested epiweek
doy_dt_epiwk_req <- lubridate::yday(date_epiweek_request)

#Modeling dates
#ending date of the FIRST epiweek in the MIN modelling year
date_min_model <- make_date_yw(year = params$year_modeling_start,
                               week = 1,
                               weekday = 7)
#ending date of LAST epiweek in the MAX modelling year
date_max_model <- make_date_yw(year = params$year_modeling_end,
                               week = get_last_epiweek(params$year_modeling_end), #could be 52 or 53
                               weekday = 7)

# date_request_week <- format(date_request, "%U") %>% as.numeric() #keeping this name, but flagging with TBC_
# 
# #<<>> EPIDATE DEV keeping old date processing for the moment until conversion to epiweeks  
# 
# # makes sure we round to the previous Sunday, so that this week is included
# TBC_weekinquestionSun <- date_request - (as.numeric(strftime(date_request, '%u')) %% 7)
# 
# TBC_weekinquestionSat <- TBC_weekinquestionSun + 6
# TBC_weekinquestionSunstr <- strftime(TBC_weekinquestionSun, '%A')
# TBC_weekinquestionSatstr <- strftime(TBC_weekinquestionSat, '%A')
# 
# TBC_weekinquestionSundoy <- as.numeric(format(TBC_weekinquestionSun, "%j"))
# 
# # figure out which is the last Sunday in the max desired year
# TBC_maxhumandesireddate <- as.Date(paste(params$year_modeling_end, "-12-31", sep=""))
# TBC_maxhumandesireddate <- TBC_maxhumandesireddate - (as.numeric(strftime(TBC_maxhumandesireddate, '%u')) %% 7)
# TBC_maxhumandesireddatestr <- strftime(TBC_maxhumandesireddate, '%A')
# 
# # figure out which is the first Sunday in the min desired human year
# TBC_minhumandesireddate <- as.Date(paste(params$year_modeling_start, "-01-01", sep=""))
# TBC_minhumandesireddate <- TBC_minhumandesireddate - (as.numeric(strftime(TBC_minhumandesireddate, '%u')) %% 7)
# TBC_minhumandesireddatestr <- strftime(TBC_minhumandesireddate, '%A')
```

```{r data_human_processing_dx, echo=FALSE, include=FALSE} 
## Human

#Record row count (in human_year_start through human_year_end)
dx_human_nrow_1range <- nrow(data_human)

#Clean human data
# remove any with unmatched district info
#   especially necessary for doing regression modeling
data_human <- data_human %>% 
  dplyr::filter(arbo_ID %in% unique(data_sf_orig$arbo_ID),
                #remove any without a good date
                (!is.na(date_obs)))

#Record row count post cleaning
dx_human_nrow_2clean <- nrow(data_human)

#gather list of districts in human data
dx_human_districts <- data_human %>% 
  pull(arbo_ID) %>% unique() %>% sort()


#[V3] [DEV] imputemissingdistricts was hard coded as FALSE in v3 production code
# updated names and code, and added dev option for renamed impute_human_missing_districts, default = FALSE
# [DEV] This looks like it samples existing districts as data for the missing districts.
#     I can't think of a time where this would be wanted. May be able to delete in future. [DMN]
if (impute_human_missing_districts){
  #list of districts in that state that are missing from human data
  districts_missing <- data_sf_orig$arbo_ID[!(data_sf_orig$arbo_ID %in% dx_human_districts)]
  #create random rows to fill in one observation for each missing district
  # by pulling a sampled row from the observed human data (for a different district) 
  sample_rows <- data_human %>% 
    dplyr::filter(row_number() %in% sample(x = 1:nrow(data_human),
                                           size = length(districts_missing),
                                           replace = TRUE))
  #pull only appropriate columns and make new tibble to add
  data_human_imputed <- tibble::tibble(arbo_ID = districts_missing) %>% 
    dplyr::bind_cols(sample_rows %>% 
                       dplyr::select(date_obs, year_epi, week_epi, date_epi, doy)) 
  #bind imputed data to existing data
  data_human <- dplyr::bind_rows(data_human, data_human_imputed)
  
}
```

```{r data_mosq_processing_dx, echo=FALSE, include=FALSE} 

## Mosquito
# Cleaning, filtering, statistics for report

#Record row count (in mosquito_year_start through mosquito_year_end)
dx_mosq_nrow_1range <- nrow(data_mosquito)

#Clean mosq data
# remove any with unmatched district info
#   especially necessary for doing regression modeling
#   Not original done in [V3] for simpleratio or AUC, 
#   but it makes sense to be consistent here. 
data_mosquito <- data_mosquito %>% 
  dplyr::filter(arbo_ID %in% unique(data_sf_orig$arbo_ID),
                #remove any with NAs in wnv_result or doy
                #both are needed in regression modeling
                #doy (and rest of date-related fields) would be na is date_obs was na
                (!is.na(wnv_result)),
                (!is.na(date_obs)))

#Record row count post cleaning
dx_mosq_nrow_2clean <- nrow(data_mosquito)

#Filter by mosquito range
data_mosquito <- data_mosquito %>% 
  #filter by doy
  dplyr::filter(doy >= params$mosquito_doy_start &
                  doy <= params$mosquito_doy_end)

#Record row count post doy filtering
dx_mosq_nrow_3filtered <- nrow(data_mosquito)

#gather list of districts in mosquito data
dx_mosq_districts <- data_mosquito %>% 
  dplyr::pull(arbo_ID) %>% unique() %>% sort()

#max/latest year diagnostics and statistics for report text and debugging
# note this is being done after mosquito_year_end filtering
#  so not latest in data file, but latest in dataset for this forecast
data_mosq_maxyr <- data_mosquito %>% 
  dplyr::filter(year_epi == max(.$year_epi, na.rm = TRUE))
#number rows of data in max year
dx_mosq_nrow_maxyr <- nrow(data_mosq_maxyr)
#number of wnv positive pools in max year
mosq_pos_num_maxyr <- data_mosq_maxyr %>% 
  dplyr::filter(wnv_result == 1) %>% 
  nrow()
#percent of pools positive in max year
mosq_pos_perc_maxyr <- mosq_pos_num_maxyr / dx_mosq_nrow_maxyr * 100 %>% 
  round(3)
```

```{r mosquito_resampling, echo=FALSE, include=FALSE} 

if (resample_mosquito){
  
  # read and process resampling file
  data_resample <- readr::read_csv(params$resample_file, show_col_types = FALSE)
  
  # added needed dates
  data_resample <- data_resample %>% 
    #try old specified format first
    #gives NA when given dates like "2019-6-4"
    dplyr::mutate(date_obs = as.Date(col_date, format = "%m/%d/%Y"),
                  #test for NA and let as.Date guess this time
                  #MUST use ifelse, not if_else b/c that evaluates all
                  # and as.Date will throw error if given the old format without pattern
                  # however ifelse strips date format, so must cast it afterwards
                  # note: zoo package used for default origin for as.Date()
                  date_obs = as.Date(ifelse(is.na(date_obs), 
                                            as.Date(col_date), 
                                            date_obs)),
                  # and doy
                  doy = as.numeric(format(date_obs, "%j")))
  
  # [V3] NOTE: Keeping original (v3) code with new names only
  # No test resample file to try
  # However, I believe it will fail as currently/previously coded, 
  #   unless mosquito data file has a different format when it needs resampling
  #   data_mosquito$total nor $positives does not exist?
  
  #create subset
  positivedoys <- data_resample$doy[data_resample$wnv_result == 1]
  negativedoys <- data_resample$doy[data_resample$wnv_result == 0]
  
  # create and fill up the expanded file
  wnv <- data.frame()
  for (i in 1:nrow(data_mosquito)) {
    
    positives <- data_mosquito$positives[i]
    negatives <- data_mosquito$total[i] - data_mosquito$positives[i]
    
    if (positives > 0) {
      
      tempdf1 <- data.frame(doy = sample(x=positivedoys,
                                         size=positives,
                                         replace=TRUE),
                            year_epi = data_mosquito$year_epi[i],
                            arbo_ID = district_shapes$arbo_ID[1], #? [V3] hard-coded first? DEV 
                            wnv_result = 1)
      
    } else { tempdf1 <- data.frame() }
    
    if (negatives > 0) {
      
      tempdf0 <- data.frame(doy = sample(x=negativedoys,
                                         size=negatives,
                                         replace=TRUE),
                            year_epi = data_mosquito$year_epi[i],
                            arbo_ID = district_shapes$arbo_ID[1], #? [V3] hard-coded first? DEV 
                            wnv_result = 0)
      
    } else { tempdf2 <- data.frame() }
    
    data_mosquito <- dplyr::bind_rows(data_mosquito, tempdf1, tempdf0)
  }
  
} #end if resample
```

```{r mosquito_infection_model, echo=FALSE, include=FALSE} 

# Calculates mir_stat (mosquito infection rate), based on mosquito model
#   Creates mosq_mir : 
#   dataset with 'year_obs' and 'mir_stat's, (and 'strata' for stratified models)
#   mir_stat_raw : calculated via modelling
#   mir_stat_ctr : post centering of above raw value
#   mir_stat : NOT created in this block, but in combining data code block below.
#                   _ctr after imputation of any missing that are needed (used in modelling)

# If match failure, default will be AUC
# Note: If add a mosquito model, you MUST add it here
if (params$mosquito_model %in% c(
  "simpleratio", 
  "AUC",
  "MIGR",
  "MII",
  "stratifiedMIGR", 
  "stratifiedMII")){
  mosquito_model_clean <- params$mosquito_model
} else {
  #if unmatched, default AUC
  mosquito_model_clean <- "AUC"
}
#list of non/stratified models for easier filtering
mosq_nonstrat_models <- c("simpleratio", "AUC", "MIGR", "MII")
mosq_strat_models <- c("stratifiedMIGR", "stratifiedMII")

# Potentially multiple if blocks per model type, 
#   depends on processing/calculations needed

###
#Primary set of calculation blocks, includes all model types
if (mosquito_model_clean == "simpleratio") {
  
  mosq_mir <- data_mosquito %>% 
    #total of positive pools, total pools, per year
    group_by(year_epi) %>% 
    dplyr::summarise(tot_pos = sum(wnv_result, na.rm=TRUE),
                     tot_test = n()) %>%
    #simpleratio MIR is ratio of total positive pools over total pools tested
    dplyr::mutate(mir_stat_raw = tot_pos / tot_test) %>% 
    #select only year and summary stat for consistency
    dplyr::select(year_epi, mir_stat_raw)
  
} #end if 'simpleratio'

if (mosquito_model_clean == "AUC") {
  
  data_mosquito <- data_mosquito %>% 
    # #remove rows with NA
    # # [V3] [DEV] note: I'm assuming glmer is unhappy otherwise [DMN]
    # dplyr::filter_at(vars(wnv_result, doy, year_obs),
    #                  #all variables listed must be not NA
    #                  all_vars(!is.na(.))) %>% 
    # [V3] create a variable that at least has a little chance of being orthogonal to 1.
    dplyr::mutate(dminus = doy - mean(.$doy, na.rm = TRUE),
                  #factor year for modeling
                  year_epi = factor(year_epi))
  
  #data frame for glmer
  data_mosq_df <- data_mosquito %>% as.data.frame()
  
  # [V3] run a random effect model on orthogonalized data
  mir_glm <- lme4::glmer(wnv_result ~ poly(dminus, 2) + (poly(dminus, 2)|year_epi),
                         family = binomial(),
                         data = data_mosq_df)
  
  # [V3] create a data frame to store the calculations for the aucs
  pred_frame <- expand.grid(year_epi = unique(data_mosquito$year_epi),
                            dminus = seq(from = min(data_mosquito$dminus, na.rm=TRUE),
                                         to = max(data_mosquito$dminus, na.rm=TRUE),
                                         length.out = 100))
  pred_frame$pred <- predict(mir_glm,
                             newdata = pred_frame,
                             type = "response")
  
  # calculate the AUCs
  # [DEV] note: Appears to be the sum of the predictions from all dminus values per year [DMN]
  mosq_mir <- pred_frame %>% 
    dplyr::group_by(year_epi) %>% 
    dplyr::summarise(mir_stat_raw = sum(pred, na.rm = TRUE)) %>% 
    #make year numeric again (not factor)
    dplyr::mutate(year_epi = as.numeric(as.character(year_epi)))
  
  #make year numeric again (not factor)
  data_mosquito <- data_mosquito %>% 
    dplyr::mutate(year_epi = as.numeric(as.character(year_epi)))
  
}

if (mosquito_model_clean %in% c("MIGR", "MII")) {
  
  data_mosquito <- data_mosquito %>% 
    # [v3] create a variable that at least has a little chance of being orthogonal to 1.
    dplyr::mutate(dminus = doy - mean(.$doy, na.rm = TRUE),
                  #factor year for modeling
                  year_epi = factor(year_epi))
  
  #data frame for glmer
  data_mosq_df <- data_mosquito %>% as.data.frame()
  
  
  #[V3] run a random effect model on orthogonalized data
  mir_glm <- lme4::glmer(wnv_result ~ 1 + dminus +
                           (0+1|year_epi) +
                           (0+dminus|year_epi),
                         family = binomial(),
                         data = data_mosq_df)
  
  ##DEV : seemingly not used again in [V3], so temporarily removing to see if we need to keep it
  #data_mosq_df$est <- predict(data_mosq_df, newdata=wnv, type="response")
  #data_mosquito <- data_mosq_df %>% tibble::as_tibble()
  
  #make year numeric again (not factor)
  data_mosquito <- data_mosquito %>% 
    dplyr::mutate(year_epi = as.numeric(as.character(year_epi))) 
  
  
  if (mosquito_model_clean == "MIGR") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(year_obs = rownames(nlme::random.effects(mir_glm)$year_epi) %>% 
                         as.numeric(),
                       #[,1] are intercept values [DMN]
                       mir_stat_raw = nlme::random.effects(mir_glm)$year_epi[,1]) 
    
    
  }
  if (mosquito_model_clean == "MII") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(year_epi = rownames(nlme::random.effects(mir_glm)$year_epi) %>%
                         as.numeric(),
                       #[,2] are dminus values [DMN]
                       mir_stat_raw = nlme::random.effects(mir_glm)$year_epi[,2]) 
    
  }
  
} #end if c("MIGR", "MII")

if (mosquito_model_clean %in% mosq_strat_models) {
  
  #add strata information to mosquito data
  data_mosquito <- data_mosquito %>% 
    dplyr::left_join(data_strata %>% 
                       select(arbo_ID, strata),
                     by = "arbo_ID") %>%
    #make sure all have a strata
    dplyr::filter(!is.na(strata)) %>% 
    # [v3] create a variable that at least has a little chance of being orthogonal to 1.
    dplyr::mutate(dminus = doy - mean(.$doy, na.rm = TRUE),
                  #create a stratum per year factor for regression
                  stratum_year = paste(strata, year_epi, sep = "_") %>% 
                    factor())
  
  #data frame for glmer
  data_mosq_df <- data_mosquito %>% as.data.frame()
  
  
  #[V3] run a random effect model on orthogonalized data
  mir_glm <- lme4::glmer(wnv_result ~ 1 + dminus +
                           (0+1|stratum_year) +
                           (0+dminus|stratum_year),
                         family = binomial(),
                         data = data_mosq_df)
  
  ##DEV : seemingly not used again in [V3], so temporarily removing to see if we need to keep it
  #data_mosq_df$est <- predict(data_mosq_df, newdata=wnv, type="response")
  #data_mosquito <- data_mosq_df %>% tibble::as_tibble()
  
  
  if (mosquito_model_clean == "stratifiedMIGR") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(stratum_year = rownames(nlme::random.effects(mir_glm)$stratum_year),
                       #[,1] seems to pull the intercept values [DMN]
                       mir_stat_raw = nlme::random.effects(mir_glm)$stratum_year[,1]) %>% 
      #split stratum and year back out
      mutate(year_epi = stringr::str_split_fixed(stratum_year, "_", n = 2)[,2],
             strata = stringr::str_split_fixed(stratum_year, "_", n = 2)[,1],
             #convert year back to numeric
             year_epi = as.numeric(year_epi),
             #need to convert strata back to whatever data type as in data_strata
             #    in example is dbl for '101', etc. But could conceivably be numeric or string
             strata = as(strata, class(data_strata$strata))) %>% 
      #drop old field
      select(-stratum_year)
    
  }
  if (mosquito_model_clean == "stratifiedMII") {
    
    #[V3] predict random effects for all years
    mosq_mir <- tibble(stratum_year = rownames(nlme::random.effects(mir_glm)$stratum_year),
                       #[,2] seems to pull the dminus values [DMN]
                       mir_stat_raw = nlme::random.effects(mir_glm)$stratum_year[,2]) %>% 
      #split stratum and year back out
      mutate(year_epi = stringr::str_split_fixed(stratum_year, "_", n = 2)[,2],
             strata = stringr::str_split_fixed(stratum_year, "_", n = 2)[,1],
             #convert year back to numeric
             year_epi = as.numeric(year_epi),
             #need to convert strata back to whatever data type as in data_strata
             #    in example is dbl for '101', etc. But could conceivably be numeric or string
             strata = as(strata, class(data_strata$strata))) %>% 
      #drop old field
      select(-stratum_year)
    
  }
  
} #end if c("stratifiedMIGR", "stratifiedMII")


# Center the MIR statistic 
# [V3] Originally done in the plotting code. Carried through to modeling, so done here. 
mosq_mir <- mosq_mir %>% 
  dplyr::mutate(mir_stat_ctr = mir_stat_raw - mean(.$mir_stat_raw, na.rm = TRUE))


# # DEV testing output
if (dev_write_output){
  readr::write_csv(mosq_mir, file = file.path(out_folder, paste0(out_name_base, "_mir_stats.csv")))
}
```

```{r data_env_latest_history, echo=FALSE, include=FALSE} 

#This block:
# 1) picks out latest values when have duplicate values for a particular day
# 2) calculates a historical env dataset on doy

# As we will likely have data from the same day in multiple files,
# we want to take only the LATEST value
# (post ID set up section to remove extraneous fields before processing)

data_env <- data_env_raw %>% 
  #trim env vars to just predictor vars (removing extra env data)
  dplyr::select(arbo_ID, date_obs, year_epi, week_epi, date_epi, doy,
                file_time, 
                params$predictor_var1, params$predictor_var2) %>% 
  # Group by county and date, 
  dplyr::group_by(arbo_ID, date_obs) %>% 
  # sort by modified time DESC, 
  dplyr::arrange(desc(file_time)) %>% 
  # slice(1) to get first/latest (or only if not duplicated)
  dplyr::slice(1) %>% 
  #drop file time
  dplyr::select(-file_time) %>%
  #ungroup to finish
  dplyr::ungroup()

#remove large and no longer needed raw dataset
rm(data_env_raw)

#Note: This does not check for missing days (IMPLICIT missing)
# If adding, adapt code from function corral_environmental()
#  in data_corrals.R in epidemia_runspace (internal repo, Github)
# However, missing days are filled in during data combining 
# Explicit missing in handled in the anomalized section, 
#   these NAs are filled the weather model predicted values

###
# Historical env dataset
# Creates a mean of the variables by day of year: "{variable name}_mean"
# This is done BEFORE any type of data filling for missing values
# [DMN] [V3] Note that in v3 the historical dataset was calculated after 
#             imputation of explicit missing in known time range
#             (which granted is no rows using gridmet data)
# Note: Anomalized historical values are added after anomalization

data_env_hx <- data_env %>%
  dplyr::group_by(arbo_ID, doy) %>%
  # [DMN] package glue's {{}} ('curly curly') 
  #       suddenly stopped working 2022-03-14. 
  #       Using older !!rlang:sym() for now, at least 
  summarize(!!rlang::sym(paste0(params$predictor_var1, 
                                "_mean")) := mean(!!rlang::sym(params$predictor_var1),
                                                  na.rm = TRUE),
            !!rlang::sym(paste0(params$predictor_var2,
                                "_mean")) := mean(!!rlang::sym(params$predictor_var2),
                                                  na.rm = TRUE),
            #   {{params$predictor_var1}} := mean({{params$predictor_var1}},
            #                                   na.rm = TRUE),
            #   {{params$predictor_var2}} := mean({{params$predictor_var2}},
            #                                   na.rm = TRUE),
            !!rlang::sym(paste0(params$predictor_var1, 
                                "_med")) := quantile(!!rlang::sym(params$predictor_var1),
                                                     probs = 0.5, na.rm = TRUE),
            !!rlang::sym(paste0(params$predictor_var2,
                                "_med")) := quantile(!!rlang::sym(params$predictor_var2),
                                                     probs = 0.5, na.rm = TRUE),
            !!rlang::sym(paste0(params$predictor_var1, 
                                "_min")) := min(!!rlang::sym(params$predictor_var1),
                                                na.rm = TRUE),
            !!rlang::sym(paste0(params$predictor_var2,
                                "_min")) := min(!!rlang::sym(params$predictor_var2),
                                                na.rm = TRUE),
            !!rlang::sym(paste0(params$predictor_var1, 
                                "_max")) := max(!!rlang::sym(params$predictor_var1),
                                                na.rm = TRUE),
            !!rlang::sym(paste0(params$predictor_var2,
                                "_max")) := max(!!rlang::sym(params$predictor_var2),
                                                na.rm = TRUE),
            .groups = "drop")

```

```{r data_env_anomalization, echo=FALSE, include=FALSE} 

# This section calculates:
# bam model for anomalization
#   generating predicted values 
#   anomaly: observed - predicted
#   Also fills in missing env data in known period with predicted values
# adds anomalized historical values to data_env_hx

# Params format reminder/example: 
#  predictor_var1: "tmeanc"
#  predictor_var2: "vpd"

#Note: field names here are dynamically created 
#   based on user parameters for var1 and var2.
#   Tidyverse programming is a little weird with that 
#     (though the syntax is cleaner now than earlier versions).
# HOWEVER, as we are dealing with dataframes (not tibbles) for bam anyway
#   using base R and [V3] code

#data frame for bam
data_env_df <- data_env %>% 
  #ID must be a factor for modeling
  mutate(arbo_ID = factor(arbo_ID)) %>% 
  as.data.frame()

#loop to do each variable
for (v in c(params$predictor_var1, params$predictor_var2)) {
  
  #set up temp variable with known name (to use in bam call)
  # will be overwritten each loop
  data_env_df$this_var <- data_env_df[,v]
  
  #run a bam model with smooth on day of year (seasonal) and district
  this_env_mod <- mgcv::bam(this_var ~ arbo_ID + s(doy, bs="cc", by=arbo_ID), 
                            data = data_env_df, 
                            discrete = TRUE)
  
  #predicted values
  this_preds <- predict(this_env_mod, newdata = data_env_df)
  
  # any missing environmental data is filled in with predicted values [V3]
  #   this means we are updating our data, so we will have to overwrite data_env 
  #   note that this is POST env modelling, but PRE forecast modelling
  data_env_df[is.na(data_env_df[,v]), v] <- this_preds[is.na(data_env_df[,v])]
  
  #anomaly calculations: deviance between observed and predicted
  #   creating a dynamically generated name that is the name of the variable plus "_anom" 
  data_env_df[, paste0(v, "_anom")] <- data_env_df[,v] - this_preds
  # then should there be any NA values, they are given the value 0 instead
  data_env_df[is.na(data_env_df[, paste0(v, "_anom")]), paste0(v, "_anom")] <- 0
  
}

#clean up and updating data_env
# changes were made of data_env_df that we need to keep
#   anomalized variables, imputed values
data_env <- data_env_df %>% 
  #convert table to tibble
  tibble::as_tibble() %>% 
  #drop temporary variable
  dplyr::select(-this_var)
#remove temporary tables to avoid confusion
rm(this_preds)
rm(data_env_df)
#remove large temporary bam model
rm(this_env_mod)

##
# Historical data
#add anomalized historical values
var1_anom_name <- paste0(params$predictor_var1, "_anom")
var2_anom_name <- paste0(params$predictor_var2, "_anom")
data_env_anom_hx <- data_env %>%
  dplyr::group_by(arbo_ID, doy) %>%
  #dynamic fields: '{var}_anom_{stat}' 
  #based on the dynamic anom fields: '{var}_anom'  
  # package glue's {{}} ('curly curly') would have been preferable to !!sym
  # e.g. {{params$predictor_var1}}, but stopped working unexpectedly [DMN]
  summarize(
    # means
    !!rlang::sym(paste0(var1_anom_name, 
                        "_mean")) := mean(!!rlang::sym(var1_anom_name), 
                                          na.rm = TRUE),
    !!rlang::sym(paste0(var2_anom_name, 
                        "_mean")) := mean(!!rlang::sym(var2_anom_name), 
                                          na.rm = TRUE),
    #medians
    !!rlang::sym(paste0(var1_anom_name, 
                        "_med")) := quantile(!!rlang::sym(var1_anom_name),
                                             probs = 0.5, na.rm = TRUE),
    !!rlang::sym(paste0(var2_anom_name, 
                        "_med")) := quantile(!!rlang::sym(var2_anom_name),
                                             probs = 0.5, na.rm = TRUE),
    #mins
    !!rlang::sym(paste0(var1_anom_name, 
                        "_min")) :=  min(!!rlang::sym(var1_anom_name),
                                         na.rm = TRUE),
    !!rlang::sym(paste0(var2_anom_name, 
                        "_min")) := min(!!rlang::sym(var2_anom_name),
                                        na.rm = TRUE),
    #maxs
    !!rlang::sym(paste0(var1_anom_name, 
                        "_max")) := max(!!rlang::sym(var1_anom_name),
                                        na.rm = TRUE),
    !!rlang::sym(paste0(var2_anom_name, 
                        "_max")) := max(!!rlang::sym(var2_anom_name),
                                        na.rm = TRUE),
    .groups = "drop")

#join to hx dataset
data_env_hx <- data_env_hx %>%
  dplyr::left_join(data_env_anom_hx, 
                   by = c("arbo_ID", "doy"))

if (dev_write_output){
  readr::write_csv(data_env_hx, file = file.path(out_folder, paste0(out_name_base, "_data_env_hx.csv")))
}

```

```{r data_combine, echo=FALSE, include=FALSE} 

# Human, mosq infection rate, and environmental data together
#   creating full matrix in prep for forecast modeling

# After creating frame based on human data, other data may need filling
#   so additional processing/calculations happen in this block

###
## Human -------------------------------------------------------

#determine the range (of weeks of year) we should include in modelling [V3]
# this is based on the human case_trim_alpha parameter
# this is removing outlier human cases that are very early or late in season [DMN]
#find min&max weeks using the case_trim_alpha parameter
human_wks_alpha <- quantile(data_human$week_epi, 
                            probs = c(params$case_trim_alpha/2, 
                                      1 - (params$case_trim_alpha/2)), na.rm = TRUE)
human_wk_min <- human_wks_alpha[1]
human_wk_max <- human_wks_alpha[2]


#Create dataset that will become the complete combined data for modelling input
#First, use human data (what we are predicting) to set start and end dates
  # need to ensure that the end week is the last week of the year of the requested forecast week, 
  #   so seq.Date is reverse chronological
  # note: no longer needed now that we are solidly on epiweeks, but doesn't impact anything so kept [DMN]
weeks_model_list <- seq.Date(from = date_max_model, to = date_min_model, by = "-1 week")
# Create a frame that has entries for all district for all weeks in modelling list
data_combined_prep <- tidyr::expand_grid(arbo_ID = dx_human_districts,
                                         date_epi = weeks_model_list) %>% 
  #add useful and needed date-related fields
  dplyr::mutate(#epi year
                year_epi = lubridate::epiyear(date_epi),
                #epiweek
                week_epi = lubridate::epiweek(date_epi),
                #also doy
                doy = lubridate::yday(date_epi),
                #create a flag if the week was in years between 
                #  params year_human_start through year_human_end
                observed = if_else(year_epi >= params$year_human_start &
                                     year_epi <= params$year_human_end,
                                   TRUE,
                                   FALSE),
                #create a flag if the week was within the trimmed alpha week range)
                modeled = if_else(week_epi >= human_wk_min &
                                    week_epi <= human_wk_max,
                                  TRUE,
                                  FALSE))

#count the number of cases that occur in each week for each district
#   note, if a district had no cases for that week, there will not be a row entry
data_human_week <- data_human %>% 
  dplyr::group_by(arbo_ID, year_epi, week_epi) %>% 
  #each row is a case
  dplyr::summarise(case_count = n(), .groups = "drop") %>% 
  #binary if any cases happened in that week in that district
  dplyr::mutate(any_cases = if_else(case_count >= 1,
                                    1, #yes, there was at least one case in this district-week 
                                    0))

#join, and fill 0s for any district-week that had no cases
data_combined_prep <- data_combined_prep %>% 
  dplyr::left_join(data_human_week, by = c("arbo_ID", "year_epi", "week_epi")) %>% 
  dplyr::mutate(case_count = tidyr::replace_na(case_count, 0),
                any_cases = tidyr::replace_na(any_cases, 0)) 


## Human case stats for report text

# Total number of cases over all districts over all time
state_cases_total <- sum(data_combined_prep$case_count, na.rm=TRUE)
state_any_cases_total  <- sum(data_combined_prep$any_cases, na.rm=TRUE)

# What percentage of cases we'll likely have seen before the start of this week
#   based on historical years
# [DEV] I believe the logic wasn't quite right in [V3]. [DMN]
#   I am adding a filter to remove the current year from the calculations, and
#   filtering by week   

cases_before_fc_wk <- data_combined_prep %>% 
  dplyr::filter(week_epi < epiweek_request) %>% 
  dplyr::pull(case_count) %>% 
  sum(na.rm = TRUE)

cases_perc_obs_hx <- 100 * (cases_before_fc_wk / state_cases_total) %>% round(2)


## Mosquito MIR -------------------------------------------------------
# mosq_mir is name of dataset from mosquito modeling with mir_stat
# mir_stat_ctr : field with mir_stat raw values that have been centered
# mir_stat : created in this block, from _ctr, but also with imputation for needed missing entries

# Create temporary frames of full data, add in mir known, flag for imputation
#   separated out by stratification models, as that involves the strata column as well 
# Using data_combined_prep (the to-be-complete combined data)
# to create a data frame with all the years that we will need for modeling

if (mosquito_model_clean %in% mosq_nonstrat_models){
  
  #for NON-stratified mosq models
  
  prep_mir_filler <- data_combined_prep %>% 
    dplyr::select(year_epi) %>% 
    #one entry for each year
    dplyr::distinct() %>% 
    #join with mosq_mir to get 'mir_stat_'s
    dplyr::left_join(mosq_mir, by = c("year_epi")) %>% 
    #add flag if MIR summary stat needs to be imputed (i.e. if that combination is missing)
    # AND if the year in within the human year range 
    dplyr::mutate(mir_impute = (is.na(mir_stat_ctr) & 
                                  year_epi >= params$year_human_start &
                                  year_epi <= params$year_human_end))
  
} else if (mosquito_model_clean %in% mosq_strat_models){
  
  #for stratified mosq models
  
  prep_mir_filler <- tidyr::expand_grid(strata = data_strata %>% 
                                          dplyr::pull(strata) %>% 
                                          unique() %>% sort(),
                                        year_epi = data_combined_prep %>% 
                                          dplyr::pull(year_epi) %>% 
                                          #for consistency with data_combined_, 
                                          #keep with reverse chronological
                                          unique() %>% sort(decreasing = TRUE)) %>% 
    #join with mosq_mir to get 'mir_stat_'s
    dplyr::left_join(mosq_mir, by = c("strata", "year_epi")) %>% 
    #add flag if MIR summary stat needs to be imputed (i.e. if that combination is missing)
    # AND if the year in within the human year range 
    dplyr::mutate(mir_impute = (is.na(mir_stat_ctr) & 
                                  year_epi >= params$year_human_start &
                                  year_epi <= params$year_human_end))
}

#replace NA mir_stat_ctr with zero (i.e. average risk, per comment from [V3])
# creating final mir_stat variable to use in modelling
prep_mir_filler <- dplyr::mutate(prep_mir_filler, 
                                 mir_stat = if_else(mir_impute, 
                                                          0,
                                                          mir_stat_ctr))

# If mir_stat has been imputed, then doing exactfit
#   Add appropriate exactfit columns here

yrs_mir_imputed <- prep_mir_filler %>% 
  dplyr::filter(mir_impute == TRUE) %>% 
  dplyr::pull(year_epi) %>% unique() %>% sort(decreasing = TRUE)

#loop over these years to add a column for each year
# that flags if it will be an exactfit
for (this_yr in yrs_mir_imputed){
  
  this_col_name <- paste0("exactfit_", this_yr)
  
  prep_mir_filler <- prep_mir_filler %>% 
    #slight weirdness for dynamic field name on left-hand side (tidyverse)
    #using glue {{}} with := assignment operator
    #https://stackoverflow.com/questions/26003574/use-dynamic-name-for-new-column-variable-in-dplyr
    dplyr::mutate({{this_col_name}} := (year_epi == this_yr))
  
}

# record years (or none) for text in report
if (length(yrs_mir_imputed) == 0) {
  yrs_mir_imputed_text <- "none"
} else {
  yrs_mir_imputed_text <- stringr::str_flatten(yrs_mir_imputed, 
                                               collapse = ", ")
}

# Now, add our final mir_stat to the combined data set
if (mosquito_model_clean %in% mosq_nonstrat_models){
  
  data_combined_prep <- data_combined_prep %>% 
    #mir_stat same for every week and district within a year
    dplyr::left_join(prep_mir_filler, by = "year_epi")
  
} else if (mosquito_model_clean %in% mosq_strat_models){
  
  data_combined_prep <- data_combined_prep %>% 
    #join with data_strata to get strata
    dplyr::left_join(data_strata %>% 
                       select(arbo_ID, strata),
                     by = "arbo_ID") %>% 
    #mir_stat same for every week and district within a year
    dplyr::left_join(prep_mir_filler, by = c("year_epi", "strata"))
  
}

if (dev_write_output){
  readr::write_csv(data_combined_prep, file = file.path(out_folder, paste0(out_name_base, "_data_combined_prep_humanmosq.csv")))
}

## Environmental -------------------------------------------------------

# fill in future and missing env data 
# within known env period, the missing have been replaced 
#   with bam predictions during anomalization 

#sequence of dates for env data matrix [V3]
# from Jan 1 of the year BEFORE the start of the requested modeling start year
# through the end of the requested modeling end year
#   Note: Creating daily rows here, but will join on weekly date below with data_combined_prep. 
#         I think we are creating more data than we need to and then discarding the rest? [DMN]
env_dates <- seq.Date(from = as.Date(paste0(params$year_modeling_start - 1, "-01-01"), 
                                     format = "%Y-%m-%d"),
                      to = as.Date(paste0(params$year_modeling_end, "-12-31"), 
                                   format = "%Y-%m-%d"),
                      by = 1)
#create blank tbl with all dates and districts (districts that were present in env data)
prep_env_filler <- tidyr::expand_grid(arbo_ID = data_env %>% 
                                        dplyr::pull(arbo_ID) %>% 
                                        unique() %>% sort(),
                                      date_obs = env_dates)


# create a new env dataset to use for modelling
#     could overwrite old one like [V3], but creating new since removing data [DMN]
# subset of env data to add to
data_env_mod <- data_env %>% 
  # select only variables we are going to use
  #using contains gets both unanomalized and anomalized columns
  dplyr::select(arbo_ID,
                date_obs,
                doy,
                tidyselect::contains(c(params$predictor_var1, 
                                       params$predictor_var2))) %>%
  #add rows from filler
  dplyr::left_join(prep_env_filler, by = c("arbo_ID", "date_obs")) %>% 
  #get historical values from hx table
  dplyr::left_join(data_env_hx, by = c("arbo_ID", "doy")) %>% 
  #replace all missing with reasonable values
  #   raw values get mean of doy
  #   anom values get 0 (no anomaly)
  #creating HARD CODED field names for bam later
  #interesting RHS syntax b/c of tidyverse
  dplyr::mutate(
    var1 = dplyr::if_else(
      #predictor_var1 from the params
      is.na(!!rlang::sym(params$predictor_var1)), 
      #if NA, use doy mean value
      !!rlang::sym(paste0(params$predictor_var1, "_mean")),
      #else use what was there
      !!rlang::sym(params$predictor_var1)),
    var2 = dplyr::if_else(
      #predictor_var2 from the params
      is.na(!!rlang::sym(params$predictor_var2)), 
      #if NA, use doy mean value
      !!rlang::sym(paste0(params$predictor_var2, "_mean")),
      #else use what was there
      !!rlang::sym(params$predictor_var2)),
    var1_anom = dplyr::if_else(
      #anomaly of predictor_var1
      is.na(!!rlang::sym(paste0(params$predictor_var1, "_anom"))), 
      #if anom is NA, use 0 (no anomaly)
      0,
      #else use what was there
      !!rlang::sym(paste0(params$predictor_var1, "_anom"))), 
    var2_anom = dplyr::if_else(
      #anomaly of predictor_var2
      is.na(!!rlang::sym(paste0(params$predictor_var2, "_anom"))),
      #if anom is NA, use 0 (no anomaly)
      0,
      #else use what was there
      !!rlang::sym(paste0(params$predictor_var2, "_anom"))))


# create the large lagged data frame
data_env_lag <- tidyr::expand_grid(arbo_ID = data_env_mod %>% 
                                     pull(arbo_ID) %>% unique(),
                                   date_obs = data_env_mod %>% 
                                     pull(date_obs) %>% unique(),
                                   #lag column
                                   lag = seq(from = 0, 
                                             to = params$lag_length,
                                             by = 1)) %>% 
  #the date of the lag (week value minus 1 day, 2 days, 3 days, etc.)
  dplyr::mutate(date_lag = date_obs - lag) %>% 
  #join with data on the LAGGED date
  dplyr::left_join(data_env_mod, 
                   by = c("arbo_ID", 
                          "date_lag" = "date_obs"))
#[DEV] note: top part of data_env_lag will be many NAs, 
# if we don't have env data prior to the start of the env data (year_weather_start)
# Fine as long as running modelling years as at least +1 of env data start

# clean up (often b/c of large data)
rm(prep_env_filler)
rm(data_env_mod)
gc(verbose = FALSE)

#turn into WIDE format
data_env_lag <- data_env_lag %>% 
  tidyr::pivot_wider(id_cols = c(arbo_ID, date_obs),
                     names_from = lag,
                     values_from = c(var1, var2, var1_anom, var2_anom))

#add into combined dataset 
# because we are going to have a column be a matrix, 
# switching to data frame (as opposed to tibble) here
# this is also necessary (data frame) for bam
data_combined <- data_combined_prep %>% 
  #adding data, here it will still be separate columns for each lag for each var
  dplyr::left_join(data_env_lag,
                   by = c("arbo_ID", "date_epi" = "date_obs")) %>% 
  as.data.frame()

#now convert into a nested matrices
#pattern matching on the var prefix followed by the day lag number
data_combined$var1 <- as.matrix(data_combined[, grep(x=colnames(data_combined),
                                                pattern="^var1_[0123456789]")])
data_combined$var2 <- as.matrix(data_combined[, grep(x=colnames(data_combined),
                                                pattern="^var2_[0123456789]")])
data_combined$var1_anom <- as.matrix(data_combined[, grep(x=colnames(data_combined),
                                                pattern="^var1_anom_[0123456789]")])
data_combined$var2_anom <- as.matrix(data_combined[, grep(x=colnames(data_combined),
                                                pattern="^var2_anom_[0123456789]")])

#now get rid of the many columns used to create the matrices
data_combined[,grep(x = colnames(data_combined), 
                    pattern="^var1_[0123456789]")] <- NULL
data_combined[,grep(x = colnames(data_combined), 
                    pattern="^var2_[0123456789]")] <- NULL
data_combined[,grep(x = colnames(data_combined), 
                    pattern="^var1_anom_[0123456789]")] <- NULL
data_combined[,grep(x = colnames(data_combined), 
                    pattern="^var2_anom_[0123456789]")] <- NULL

#create a matrix for the lag
#using var1 just as a frame
data_combined$lag <- data_combined$var1
#rename (though not necessary, just for clarity)
colnames(data_combined$lag) <- gsub(x = colnames(data_combined$lag),
                                    pattern = "var1",
                                    replacement = "lag")
#now rewrite values
for (i in 1:ncol(data_combined$lag)){
  #starting at 0, so i-1
  data_combined$lag[, i] <- (i - 1)
}

#need terms for seasonally-varying distributed lag
data_combined$doymat <- data_combined$lag
data_combined$doymat[,] <- data_combined$doy #[V3] uncertain what this does [DMN]

# clean up (often b/c of large data)
rm(data_env_lag)
gc(verbose = FALSE)

## Post-combining ---------------------------------------------------

#store arbo_ID data type to unfactor later
arbo_ID_class <- class(data_combined$arbo_ID)

#ensure that arbo_ID is a factor (needed for bam)
data_combined <- data_combined %>% 
  dplyr::mutate(arbo_ID = factor(arbo_ID))

```

```{r forecast_modeling, echo=FALSE, include=FALSE}

# This code block does the forecast modeling
# Switches for if saving model objects

#Function from plyr with no dplyr equivalent
round_any <- function(x, accuracy, f = round){
  f(x / accuracy) * accuracy
}

if (save_models){
  #set up list to contain model objects (created cached models)
  model_objs <- list()
}

#set up frame to collect evaluation statistics 
# note that this is ALWAYS done, whether or not dev parameter is set
# because a subset of these is in the report itself
model_evals <- data.frame()

# set up list to contain model plots
# [DEV] this is the FIRST place plots are generated
#       this is being done in the loop below because unless the
#       models are being cached, they are not saved so generating 
#       these plots after the loop would be impossible
model_plots <- list()

# set up frame to capture predictions
preds <- data.frame()

# using list of models, run each one
for (this_model in model_names){
  
  ##
  # formula set up
  #
  #get the formula
  this_formula_raw <- model_formulas[this_model]
  
  #add any exact fits
  exactfit_list <- colnames(data_combined)[grep(x = colnames(data_combined),
                                             pattern = "exactfit_",
                                             fixed = TRUE)]
  if (length(exactfit_list) > 0) {
    #create formula terms
    exactfit_list <- paste(exactfit_list, collapse = " + ")
    #update formula with new pieces
    this_formula_raw <- paste(this_formula_raw, exactfit_list, sep= " + ")
    
  } #end exactfit additions
  
  #final formula (includes any exact fits)
  this_formula <- as.formula(this_formula_raw)

  ##
  # forecast human risk model / regression
  #
  #USE cached model if given, if it's a lm object
  if (!is_null(models_cached)){ 
    
    #very basic test if reg object. bam: bam, gam, glm, lm
    if (is(models_cached[[c(this_model)]], "lm")){
    #get right math model regression object
    this_reg <- models_cached[[c(this_model)]]
    } #end if reg obj
  
  } else {
    #if not given a cached model, or cached model given was not an lm,
    # create model
    cl <- parallel::makeCluster(parallel::detectCores(logical = FALSE) - 1)
    
    #switch for regression type 
    #[DEV] hook only, left open for development later [DMN]
    if (reg_function == "GAM"){
      this_reg  <- mgcv::bam(formula = this_formula,
                       family = binomial(), 
                       data = data_combined,
                       subset = modeled == 1,
                       cluster = cl)
      
    } #end regression switch
    
    parallel::stopCluster(cl)
    
  } #end cached model/create model

  # if saving models, add model obj to list
  if (save_models){
    model_objs[[this_model]] <- this_reg
  }

  # add model plots to the list
  # <<>> DEV May need to make better plots? find out where used in report and how it is formatted
  model_plots[[this_model]] <- plot(this_reg, select=1)

  ##
  # predictions of this model
  #
  
  # predict on this model
  # note that this field will be overwritten each model in loop
  data_combined$pred <- predict(this_reg, 
                                newdata = data_combined, 
                                newlevels=TRUE, type="response")

  #censor predictions outside of the alpha window for human cases 
  # done in [V3] but with a confusing code comment, but this is what it does [DMN]
  data_combined$pred[data_combined$week_epi < human_wk_min] <- 0
  data_combined$pred[data_combined$week_epi > human_wk_max] <- 0

  #add predictions from the model to all model prediction data frame
  # first generate a temporary data frame with just the data to add
  pred_prep <- data.frame(arbo_ID = data_combined$arbo_ID,
                          date_epi = data_combined$date_epi,
                          week_epi = data_combined$week_epi,
                          year_epi = data_combined$year_epi, 
                          any_cases = data_combined$any_cases,
                          case_count = data_combined$case_count,
                          observed = data_combined$observed,
                          modeled = data_combined$modeled,
                          pred = data_combined$pred,
                          model = this_model)
  preds <- dplyr::bind_rows(preds, pred_prep)
  
  
  ##
  # evaluate this model fit
  #
  eval_prep <- data_combined[data_combined$modeled == 1,]
  #for display in report, auc and aic should be rounded (as opposed to more digits here)
  eval_prep <- data.frame(auc = round_any(pROC::roc(response = eval_prep$any_cases,
                                                         predictor = eval_prep$pred,
                                                         na.rm=TRUE,
                                                         auc=TRUE)$auc, 0.00001),
                          aic = round_any(AIC(this_reg)[1], 0.00001),
                          model = this_model)
  model_evals <- dplyr::bind_rows(model_evals, eval_prep)
  
} #end run each model

# convert results data into tbls for ease later in plotting
#  and return arbo_ID to original type
preds <- tibble::as_tibble(preds) %>% 
  dplyr::mutate(arbo_ID = as.character(arbo_ID) %>% 
                  as(arbo_ID_class),
                #also add doy of date_epi, used in graphing
                doy = lubridate::yday(date_epi))
model_evals <- tibble::as_tibble(model_evals)

# if saving models, name and save out
if (save_models){
    saveRDS(model_objs, file = file.path(out_folder, paste0(out_name_base, ".rds")))
}

if (dev_write_output){
  readr::write_csv(preds, file = file.path(out_folder, paste0(out_name_base, "_predictions.csv")))
  readr::write_csv(model_evals, file = file.path(out_folder, paste0(out_name_base, "_evals.csv")))
}
```

<!-- End of internals: data load, processing, forecast modeling, etc. -->
<!-- Everything below is report: text and figures. -->
<!-- Division 1: Forecast Results -->
<!-- 1.1 Relative risk map [modified] -->
<!-- 1.2 Absolute risk map [condensed] -->
<!-- 1.3 Current-year fc chart [mean w/ max/min] PLUS human hx epicurve -->
<!-- 1.4 Positives-to-cases [condensed: mean w/ range] -->
<!-- 1.5 Multi-year forecasting chart [mean w/ max/min] -->

<!-- Division 2: Input Data -->
<!-- 2.1 Mosquito pools [with new] -->
<!-- 2.2 Weather [observed] -->
<!-- 2.3 Human cases -->
<!-- 2.4 Data used descriptions -->
<!-- 2.5 Spatial data -->
<!-- 2.6 Run parameters -->
<!-- 2.7 Reference map -->

# Forecast Results

```{r common_themes, echo=FALSE, include=FALSE}

# common map themes
theme_arbo_map <- theme_void() 
# theme()

# common chart themes
theme_arbo_chart <- theme_bw() +
  theme(
    # grid
    panel.grid.major=element_blank(), 
    panel.grid.minor=element_blank(),
  )

#Named list of short model descriptions / better names
# includes large set of potential names, less with be used in the actual reports most likely
model_desc_names <- c(
  "cub-fx-nonanom" = "Non-anomalized weather with fixed cubic splines",
  "cub-fx-anom" = "Anomalized weather with fixed cubic splines",
  "cub-sv-nonanom" = "Non-anomalized weather with seasonally-varying cubic splines",
  "cub-sv-anom" = "Non-anomalized weather with seasonally-varying cubic splines",
  "tp-fx-nonanom" = "Non-anomalized weather with fixed thin plate splines",
  "tp-fx-anom" = "Anomalized weather with fixed thin plate splines",
  "tp-sv-nonanom" = "Non-anomalized weather with seasonally-varying thin plate splines",
  "tp-sv-anom" = "Anomalized weather with seasonally-varying thin plate splines")
#get additional <<>> DEV

```


## Current-week WNV relative risk map

```{r relative_risk_map, echo=FALSE, include=TRUE, fig.align = 'center', fig.width = 7}

# Want to compare the current risk to the historical/previous years

# [V3] I've tried to be faithful to the algorithm from v3, 
#       but there were no why-comments, so I had to try to logic it out [DMN]
# [V3] The following is the v3 algorithm, below this is the new V4 algorithm:
# V3.1. Take the mean of the predictions across all models for each district-week
#     Note: using the preds_ave tbl created in forecast-summary code block
# V3.2. If there are more than 1 predictions in a district-year, 
#     (just as a check that the calculation will work) 
#     then find the PRED value that would have occurred approximately 
#       at the SAME DOY as the current forecast week, for each district-year
#       (using the ave pred values from step 1.)  
#       via a linear interpolation (using DOY)
# V3.3. RANK these pred values (~DOY-of-forecast model-average pred values for each district-year)
#     and divide by the number of years of values each district has.
#     This creates a ~percentile of RANKED risk (basically)
# V3.4. Break percentile into categories

# V4 algorithm
# 1. Take the mean of the predictions across all models for each district-week
#     Note: using the preds_ave tbl created in forecast-summary code block
# 2. For each district-year, get the pred value at the same EPI WEEK as the forecast week
# 3. RANK these pred values and divide by the number of years of values each district has.
#     This creates a ~percentile of RANKED risk (basically)
# 4. Percentile values kept as continuous to match style of absolute risk maps
#     [V3]: <= 12.5% lower than ave, => 87.5% higher than ave
#     Will need to indicate something similar in legend
# Note: This is therefore the relative risk of PREDICTED values. 
#     May want to consider if there is some way to compare forecast-week predicted value
#       against OBSERVED historical values (any_cases), but not certain how that 
#       mathematically would work out (as any_cases is {0|1}). [DMN]

## Average predictions 
# average predictions of all models per district
preds_mod_ave <- preds %>% 
  #specifically NOT grouping by 'model'
  dplyr::group_by(arbo_ID, 
                  #keeping all date-related fields, 
                  # need to at least group on date_epi (or year+week)
                  date_epi, week_epi, year_epi, doy) %>% 
  dplyr::summarise(pred_ave = mean(pred, na.rm = TRUE),
                   #some stats of all models for graphing
                   pred_min = min(pred, na.rm = TRUE),
                   pred_max = max(pred, na.rm = TRUE),
                   #rest are same across models, taking first to keep
                   #could have grouped by and gotten same result
                   any_cases = first(any_cases),
                   case_count = first(case_count),
                   observed = first(observed),
                   modeled = first(modeled),
                   .groups = 'drop')


rel_risk <- preds_mod_ave %>% 
  #get all ave-preds at same epiweek (in any year)
  #   now only 1 row per year
  dplyr::filter(week_epi == epiweek_request) %>% 
  # rank within each district
  dplyr::group_by(arbo_ID) %>% 
  dplyr::mutate(rank_pred = rank(pred_ave, ties.method = 'random'),
                #percentile of ranked pred by number of years
                # note: rank such that lowest pred = 1
                rank_perc = rank_pred / n() * 100) %>% 
  #and get this current-year week
  dplyr::filter(year_epi == epiyear_request) %>% 
  #ungroup to finish
  dplyr::ungroup()

#join with spatial
sf_rel_risk <- data_sf_orig %>% 
  dplyr::left_join(rel_risk, by = "arbo_ID")

#plot
#same breaks as [V3]
lower_risk <- 12.5
higher_risk <- 87.5
p_rel_risk <- ggplot2::ggplot() +
  #by rank percentile
  ggplot2::geom_sf(data = sf_rel_risk,
                   aes(fill = rank_perc)) +
  viridis::scale_fill_viridis("", #"Risk\n", #\n for extra vertical space
                              #limits always 0 - 100
                              limits = c(0, 100),
                              #using [V3] breaks
                              breaks = c(0, lower_risk, 50, higher_risk, 100),
                              labels = c("0",
                                         paste(lower_risk, ": Lower than average risk"),
                                         "50: Average risk",
                                         paste(higher_risk, ": Higher than average risk"),
                                         "100")) +
  ggtitle("Risk in forecast week relative to the same epiweek in previous years") +
  theme_arbo_map

plot(p_rel_risk)


#Table of counties with higher than average risk
knitr::kable(rel_risk %>% 
  #<<>>  >= HIGHER_RISK, but testing  DEV
  dplyr::filter(pred_ave >= higher_risk) %>% 
  #join with crosswalk to get pretty names
  dplyr::left_join(id_crosswalk, by = "arbo_ID") %>% 
    dplyr::select(NAME, pred_ave),
             col.names = c("Name", "Risk"),
             digits = 2,
             caption = "Counties with higher than average risk")


knitr::kable(rel_risk %>% 
  #<<>>  >= HIGHER_RISK, but testing  DEV
  dplyr::filter(pred_ave < higher_risk) %>% 
  #join with crosswalk to get pretty names
  dplyr::left_join(id_crosswalk, by = "arbo_ID") %>% 
    dplyr::select(NAME, pred_ave),
             col.names = c("Name", "Risk"),
             digits = 2,
             caption = "Counties with higher than average risk",
             label = "DEV <<>> LOWER THEN HIGHER TO TEST")


```

## Current-week WNV absolute risk map

```{r absolute_risk_map, fig.width=7, echo=FALSE, include=TRUE}

# Creates a map of absolute risk of the requested forecast week
# Average of all models here (each model in appendix)

# average predictions for all models per district 
#   in the forecast week (and year)
preds_mod_ave_fc_wk <- preds_mod_ave %>% 
  dplyr::filter(year_epi == epiyear_request & week_epi == epiweek_request)

#join with spatial
sf_abs_risk <- data_sf_orig %>% 
  dplyr::left_join(preds_mod_ave_fc_wk, by = "arbo_ID")

p_abs_risk <- ggplot2::ggplot() +
  ggplot2::geom_sf(data = sf_abs_risk,
                   aes(fill = pred_ave)) +
  viridis::scale_fill_viridis("", #"Risk\n", #\n for extra vertical space
                              #limits always 0 - 1
                              limits = c(0, 1),
                              breaks = c(0, 1),
                              labels = c("0: Less likely to report any cases", 
                                         "1: More likety to report at least one case")) +
  ggtitle("Absolute risk in forecast week") +
  theme_arbo_map

plot(p_abs_risk)

```

## Current-year forecast

```{r current_year_forecast, fig.width=7, echo=FALSE, include=TRUE}

#Creates an epicurve of the current forecast year PREDICTIONS
# Average of models, with ribbon for min/max models
# Adds the HISTORICAL human epicurve 
#
# Note: individual model version moved to Appendix

# [DEV] Note that human summary data is on a different scale
#   than forecast, and ggplot2 does NOT allow for 2 different x-axes
# Rescaled by using the average historical proportion positive
#   as opposed to the [V3] count of districts positive

# <<>> DEV!!
# 2018 is quite different with new version?? But other years look relatively the same...


## Statewide average predictions
# averaging to state first
preds_st_ave <- preds %>% 
  #not grouping by arbo_ID here
  dplyr::group_by(model, 
                  #keeping all date-related fields, 
                  # need to at least group on date_epi (or year+week)
                  date_epi, week_epi, year_epi, doy) %>% 
  dplyr::summarise(pred_ave = mean(pred, na.rm = TRUE),
                   mean_any_cases = mean(any_cases, na.rm = TRUE),
                   tot_case_count = sum(case_count, na.rm = TRUE), 
                   .groups = 'drop')
# now averaging by model, grouping just on date fields
preds_st_mod_ave <- preds_st_ave %>% 
  dplyr::group_by(date_epi, week_epi, year_epi, doy) %>% 
  dplyr::summarise(pred_mod_ave = mean(pred_ave, na.rm = TRUE),
                   #some stats of all models for graphing
                   pred_mod_min = min(pred_ave, na.rm = TRUE),
                   pred_mod_max = max(pred_ave, na.rm = TRUE),
                   #rest are same for each model, taking first to keep
                   mean_any_cases = first(mean_any_cases),
                   tot_case_count = first(tot_case_count),
                   .groups = 'drop') %>% 
  #split into two series : pre & post fc_week - for graphing with two different styles
  #intentionally having fc week be both pre and post, so lines appear connected
  dplyr::mutate(pred_mod_ave_pre = if_else(year_epi < epiyear_request, #previous years
                                           pred_mod_ave,
                                           NA_real_),
                #earlier in forecast yr, update 
                pred_mod_ave_pre = if_else(week_epi <= epiweek_request & year_epi == epiyear_request, 
                                           pred_mod_ave,
                                           pred_mod_ave_pre),
                pred_mod_ave_post = if_else(week_epi >= epiweek_request & year_epi == epiyear_request,
                                            pred_mod_ave,
                                            NA_real_)) 

#only forecast year (statewide average model predictions)
preds_st_mod_ave_fc_yr <- preds_st_mod_ave %>% 
  dplyr::filter(year_epi == epiyear_request) 

#censored to modelled range (with buffer)
preds_st_mod_ave_fc_yr_censor <- preds_st_mod_ave_fc_yr %>% 
  #censoring to just modeled period WITH buffer
  dplyr::filter(week_epi >= (human_wk_min - 1) &
                  week_epi <= (human_wk_max + 1))

data_human_st_prop <- data_combined %>%
  #proportion of districts positive in each year by week
  dplyr::group_by(year_epi, week_epi) %>%
  dplyr::summarise(tot_any = sum(any_cases, na.rm = TRUE),
                   prop_pos = tot_any / length(dx_human_districts),
                   .groups = 'drop') %>%
  #summary stats by week
  dplyr::group_by(week_epi) %>%
  dplyr::summarise(pp_mean = mean(prop_pos, na.rm = TRUE),
                   pp_med = median(prop_pos, na.rm = TRUE),
                   pp_min = min(prop_pos, na.rm = TRUE),
                   pp_max = max(prop_pos, na.rm = TRUE))
# #DEV quick check
# ggplot() + 
#    geom_line(data = data_human_st_prop %>% 
#                tidyr::pivot_longer(c(pp_mean, pp_med, pp_min, pp_max), 
#                                    names_to = "stat", values_to = "value"), 
#              aes(x=week_epi, y = value, color = stat))

#Combined data, long format for ease in plotting, weird with ribbon data
data_cur_yr <- preds_st_mod_ave_fc_yr_censor %>% 
  #get human data 
  dplyr::left_join(data_human_st_prop %>% 
                     #var for plotting
                     dplyr::mutate(line_style = "human"), 
                   by = "week_epi") %>%  
  #pivot long for ggplot
  dplyr::select(week_epi, pred_mod_ave_pre, pred_mod_ave_post, pred_mod_min, pred_mod_max, pp_mean) %>% 
  tidyr::pivot_longer(cols = c(pred_mod_ave_pre,
                               pred_mod_ave_post,
                               pp_mean),
                      names_to = "series",
                      values_to = "stat_value") %>% 
  #change order in legend
  dplyr::mutate(series = factor(series, levels = c("pp_mean", "pred_mod_ave_pre", "pred_mod_ave_post")))

#Current year forecast plot
p_cur_yr <- ggplot2::ggplot(data = data_cur_yr) +
  #min/max ribbon first so that ave pred plots on top
  geom_ribbon(aes(x = week_epi,
                  ymin = pred_mod_min,
                  ymax = pred_mod_max,
                  fill = "model_range",
                  alpha = "model_range")) +
  geom_line(aes(x = week_epi,
                y = stat_value,
                linetype = series,
                color = series)) +
  #current week marker
  geom_vline(xintercept = epiweek_request, linetype="dashed", color = "grey25") +
  #plot labels and adjustments
  #linetype and color match to keep line series in one legend
  scale_linetype_manual("",
                        values = c("pp_mean" = "solid", 
                                   "pred_mod_ave_pre" = "solid",
                                   "pred_mod_ave_post" = 22), #22 is tight dashed
                        labels = c("pp_mean" = "Historical average proportion", 
                                   "pred_mod_ave_pre" = "Past forecast",
                                   "pred_mod_ave_post" = "Future forecast")) +
  scale_color_manual("",
                     values = c("pp_mean" = "darkblue", 
                                "pred_mod_ave_pre" = "darkred",
                                "pred_mod_ave_post" = "darkred"), 
                     labels = c("pp_mean" = "Historical average proportion", 
                                "pred_mod_ave_pre" = "Past forecast",
                                "pred_mod_ave_post" = "Future forecast")) +
  #fill and alpha match to keep ribbon in one legend
  scale_fill_manual("", 
                    values = c("model_range" = "darkred"),
                    labels = "Range of all models") +
  scale_alpha_manual("", 
                     values = c("model_range" = 0.15),
                     labels = "Range of all models") +
  ggtitle(paste("Statewide model predictions in", epiyear_request, "at forecast epiweek", epiweek_request)) + 
  #[V3] How y-axis labeled in v3, keeping same [DMN]
  ylab("Proportion of districts positive") + 
  xlab("Epiweek") +
  theme_arbo_chart 

plot(p_cur_yr)

#Test to see old V3 epi curve in comparison
data_human_st <- data_combined %>%
  #proportion of districts positive by week
  dplyr::group_by(week_epi) %>%
  dplyr::summarise(tot_any = sum(any_cases, na.rm = TRUE),
                   .groups = 'drop')
p_human_curve <- ggplot() +
  geom_line(data = data_human_st %>%
                       #censoring to same just modeled period WITH buffer
                         dplyr::filter(week_epi >= (human_wk_min - 1) &
                                         week_epi <= (human_wk_max + 1)),
            aes(x = week_epi, y = tot_any)) +
  #current week marker
  geom_vline(xintercept = epiweek_request, linetype="dashed", color = "grey75") +
  ggtitle("DEV TEST <<>> [V3] Districts with any number of cases, over all years") +
  ylab("Count of districts") +
  xlab("Epiweek") +
  theme_arbo_chart
plot(p_human_curve)


# <<>> DEV Adding per model version here for the moment to see (will move to appendix)
preds_st_ave_yr <- preds_st_ave %>% 
  dplyr::filter(year_epi == epiyear_request) %>% 
  #mark which are forecast week and beyond (i.e. future)
  dplyr::mutate(post_fc_wk = if_else(week_epi >= epiweek_request, 
                                     TRUE,
                                     FALSE))

p_apx_cur_yr <- ggplot2::ggplot() +
  #model predicted values 
  ggplot2::geom_line(data = preds_st_ave_yr %>% 
                       #censoring to just modeled period WITH buffer
                       #  (between human min/max weeks from alpha calc)
                       dplyr::filter(week_epi >= (human_wk_min - 1) &
                                       week_epi <= human_wk_max + 1) %>% 
                       #need to duplicate fc week with post_fc_wk flag set to false 
                       #so that lines connect
                       dplyr::bind_rows(preds_st_ave_yr %>% 
                                          dplyr::filter(week_epi == epiweek_request) %>% 
                                          dplyr::mutate(post_fc_wk = FALSE)), 
                     aes(x = week_epi,
                         y = pred_ave,
                         linetype = post_fc_wk,
                         color = model)) +
  #plot labels and adjustments
  ggtitle(paste("TEMPORARY DEV <<>> Statewide model predictions in", epiyear_request)) + 
  #[V3] How y-axis labeled in v3, keeping same [DMN]
  ylab("Proportion of districts positive") +
  xlab("Epiweek") +
  theme_arbo_chart

plot(p_apx_cur_yr)


# DEV code graveyard
  # geom_label(aes(label = paste("Forecast Week: ", epiweek_request), 
  #                x = epiweek_request, 
  #                y = Inf),
  #            fill = "white") +
  # coord_cartesian(clip = "off") +


```

## Case estimation

```{r positives_to_cases, fig.width=7, echo=FALSE, include=TRUE}

#Estimates number of cases from positive district-weeks
# Splits out previous yearly summaries to model
#   and predicts on current year

#calculate yearly summaries
pred_yrs <- preds %>% 
  #summing all districts, all weeks in year
  dplyr::group_by(year_epi, model) %>% 
  dplyr::summarise(tot_any = sum(any_cases, na.rm = TRUE),
                   tot_pred = sum(pred, na.rm = TRUE), #v3 totest temp dev note
                   tot_cases = sum(case_count, na.rm = TRUE),
                   .groups = 'drop') %>% 
  #[V3] 
  dplyr::mutate(weight = 1)

#this forecast year
pred_yrs_fc_yr <- pred_yrs %>% 
  dplyr::filter(year_epi == epiyear_request) %>% 
  #cases not known in forecast year, copy over predictions to be used to predict
  dplyr::mutate(tot_any = tot_pred)

#all previous years
pred_yrs_pre_yr <- pred_yrs %>% 
  dplyr::filter(year_epi < epiyear_request) %>% 
  #bind these rows, as in [V3]
  dplyr::bind_rows(tidyr::expand_grid(tot_any = 0,
                                      tot_cases = 0,
                                      weight = 100,
                                      model = unique(pred_yrs$model))) %>% 
  #make sure model is a factor (necessary for regression)
  dplyr::mutate(model = factor(model))


# [V3] figure out relationship between positivity and total cases
pos_reg <- lm(tot_cases ~ poly(x = tot_any, degree = 2), 
              weights = weight, 
              data = pred_yrs_pre_yr %>% as.data.frame())

#create a framework to put predictions into 
# predicting a range from 0 to the max number of observed total of any_cases for a year
# used in graph in [V3], may move to appendix, may just refer to this dataset from there # DEV <<>>
pos_cases_pred <- expand.grid(tot_any = seq(from = 0,
                                      to = max(pred_yrs$tot_any, na.rm=TRUE),
                                      #creates seq of equally spaced values from 'from' to 'to'
                                      length.out = 50),
                         model = unique(pred_yrs$model)) %>% 
  tibble::as_tibble()
pos_cases_pred$tot_pred_cases <- predict(pos_reg, newdata = pos_cases_pred)

# predict past cases for comparison
pred_yrs_pre_yr$tot_pred_cases <- predict(pos_reg, newdata = pred_yrs_pre_yr)
# and predict this year
pred_yrs_fc_yr$tot_pred_cases <- predict(pos_reg, newdata = pred_yrs_fc_yr)

# DEV <<>>
# was going to look at listing all years, but check into why all previous years, the total numbers (any, pred, cases, pred_cases) are all the same across models

#average model (in main report)
pos_case_preds_ave <- pred_yrs_fc_yr %>% 
  dplyr::group_by(year_epi) %>% 
  #current year forecast prediction and estimated total cases
  dplyr::summarise(ave_tot_pred = mean(tot_pred, na.rm = TRUE),
                   ave_tot_pred_cases = mean(tot_pred_cases, na.rm = TRUE))

#create table to display
knitr::kable(pos_case_preds_ave %>% 
               dplyr::select(year_epi, ave_tot_pred, ave_tot_pred_cases),
             col.names = c("Year", "Predicted positive district-weeks", "Estimated total cases"),
             digits = 0, align = "c")


```

## Multi-year forecast

```{r multi_year_forecast, fig.width=7, echo=FALSE, include=TRUE}

#Creates a multi-year time series chart of the predicted (model average) vs. 
#   observed positive district-weeks

multiyr <- preds_st_mod_ave %>% #tbl from beginning of current_year_forecast code block
  #update mean_any_cases to be NA during forecast year
  dplyr::mutate(mean_any_cases = if_else(year_epi == epiyear_request,
                                         NA_real_,
                                         mean_any_cases)) %>% 
  #censoring to just modeled period WITH buffer
  # otherwise graph has long unmodelled/0 stretches
  dplyr::filter(week_epi >= (human_wk_min - 1) &
                  week_epi <= (human_wk_max + 1)) %>% 
  #modified date to deal with skipping weeks between seasons
  # borrowed from [V3]
  # year + decimal fraction [DMN]
  dplyr::mutate(modwk = (week_epi - human_wk_min) / (human_wk_max - human_wk_min + 1),
                moddate = year_epi + modwk)

#learning from current year graph, splitting into two datasets 
# for 1) ribbon and 2) the line series
multiyr_wide <- multiyr %>% 
  #pull needed 
  dplyr::select(moddate, 
                pred_mod_min, pred_mod_max)

multiyr_long <- multiyr  %>% 
  #pull needed and pivot long
  dplyr::select(moddate, 
                pred_mod_ave_pre, pred_mod_ave_post, mean_any_cases) %>% 
  tidyr::pivot_longer(cols = c(pred_mod_ave_pre, pred_mod_ave_post, mean_any_cases),
                               names_to = "series",
                               values_to = "value")


p_multiyr <- ggplot() +
  #min/max ribbon first so that ave pred plots on top
  geom_ribbon(data = multiyr_wide,
              aes(x = moddate,
                  ymin = pred_mod_min,
                  ymax = pred_mod_max,
                  fill = "model_range",
                  alpha = "model_range")) +
  geom_line(data = multiyr_long,
            aes(x = moddate, 
                y = value,
                color = series,
                linetype = series)) +
  #plot labels and adjustments
  #linetype and color match to keep line series in one legend
  scale_linetype_manual("",
                        values = c("mean_any_cases" = "solid", 
                                   "pred_mod_ave_pre" = "solid",
                                   "pred_mod_ave_post" = 22), #22 is tight dashed
                        labels = c("mean_any_cases" = "Observed", 
                                   "pred_mod_ave_pre" = "Past forecast",
                                   "pred_mod_ave_post" = "Future forecast")) +
  scale_color_manual("",
                     values = c("mean_any_cases" = "black", 
                                "pred_mod_ave_pre" = "darkred",
                                "pred_mod_ave_post" = "darkred"), 
                     labels = c("mean_any_cases" = "Observed",  
                                "pred_mod_ave_pre" = "Past forecast",
                                "pred_mod_ave_post" = "Future forecast")) +
  #fill and alpha match to keep ribbon in one legend
  scale_fill_manual("", 
                    values = c("model_range" = "darkred"),
                    labels = "Range of all models") +
  scale_alpha_manual("", 
                     values = c("model_range" = 0.15),
                     labels = "Range of all models") +
  #x-axis management to only show the modelled weeks per year 
  #   borrowed heavily from [V3]
  scale_x_continuous(breaks = seq(from = params$year_human_start + 0.5,
                                to = (params$year_human_end + 1.5),
                                by = 1),
                     labels = seq(from = params$year_human_start, 
                                  to = params$year_human_end + 1, 
                                  by = 1),
                     limits = c((params$year_human_start),
                                (params$year_human_end + 2))) +
  ggtitle(paste("Statewide model predictions")) + 
  #[V3] How y-axis labeled in v3, keeping same [DMN]
  ylab("Proportion of districts positive") + 
  xlab("Epiweek") +
  theme_arbo_chart 

plot(p_multiyr)

## quick & dirty
d_multiyr <- preds %>% 
  dplyr::filter(week_epi >= (human_wk_min - 1) &
                  week_epi <= (human_wk_max + 1)) %>% 
  group_by(date_epi, model) %>% 
  summarize(obs = mean(any_cases, na.rm = TRUE),
            est = mean(pred, na.rm = TRUE),
            week_epi = mean(week_epi, na.rm = TRUE),
            .groups = 'drop')
thisplot <- ggplot() + 
  geom_line(data=d_multiyr, aes(x=date_epi, y=obs, group=model)) +
  geom_line(data=d_multiyr, aes(x=date_epi, y=est, group=model, color=model, linetype=model)) +
  #geom_line(data=tempdf3, aes(x=newdate, y=est, group=model, color=model), linetype=1, show.legend=FALSE) +
  #scale_color_manual(values = mycolors) +
  #scale_linetype_manual(values = mylinetypes) +
  ggtitle("DEV TEMPORARY <<>> Statewide model predictions") +
  xlab("") + ylab("") +
  #scale_x_continuous(breaks=seq(from=minhumandesiredyear+0.5,to=(maxhumandesiredyear+0.5),by=1),
  #                   labels=seq(from=minhumandesiredyear, to=maxhumandesiredyear, by=1),
  #                   limits=c((minhumandesiredyear),(maxhumandesiredyear+1))) +
  ylab("Proportion of districts positive") +
  theme(panel.grid.minor.x=element_blank()) +
  theme(legend.position="bottom") +
  theme(panel.grid.minor = element_blank(), panel.background = element_blank(),
        legend.key = element_blank(), text=element_text(size=10))
thisplot



```

# Data summaries

DEV: Printing a bunch of things - 
dx_mosq_nrow_0load: `r dx_mosq_nrow_0load`
dx_human_nrow_0load: `r dx_human_nrow_0load`
dx_env_nrow_0load, note PRE de-duplication: `r dx_env_nrow_0load`
dx_human_nrow_1range: `r dx_human_nrow_1range`
dx_human_nrow_2clean: `r dx_human_nrow_2clean`
dx_human_districts: `r dx_human_districts`
state_cases_total: `r state_cases_total`
state_any_cases_total: `r state_any_cases_total`
cases_before_fc_wk: `r cases_before_fc_wk`
cases_perc_obs_hx: `r cases_perc_obs_hx`
dx_mosq_nrow_1range: `r dx_mosq_nrow_1range` 
dx_mosq_nrow_2clean: `r dx_mosq_nrow_2clean`
dx_mosq_nrow_3filtered: `r dx_mosq_nrow_3filtered`
dx_mosq_districts: `r dx_mosq_districts`
dx_mosq_nrow_maxyr: `r dx_mosq_nrow_maxyr`
mosq_pos_num_maxyr: `r mosq_pos_num_maxyr`
mosq_pos_perc_maxyr: `r mosq_pos_perc_maxyr`




## Mosquito pools

```{r mosquito_pools, fig.width=7, fig.height=3, echo=FALSE, include=TRUE}



```

## Weather

```{r weather_charts, fig.width=7, fig.height=3, echo=FALSE, include=TRUE}

p_env_var1 <- ggplot() + 
  #historical min/max
  geom_line(data=data_env_hx, 
            aes(x=doy, y=!!rlang::sym(paste0(params$predictor_var1, "_mean")))) +
  #geom_ribbon(data=doymet, 
  #            aes(x=doy, ymin=min_var1, ymax=max_var1), alpha=0.3) +
  #observed
  geom_line(data=data_env %>% filter(year_epi == epiyear_request), 
            aes(x=doy, y=!!rlang::sym(params$predictor_var1)), 
            color="red", size=1) +
  xlab("Day of the year") + ylab(params$predictor_var1) +
  ggtitle(paste(params$predictor_var1, 
                max(data_env$year_epi, na.rm=TRUE),
                "FIXME_DEV_<<>>",
                sep=" "))
p_env_var2 <- ggplot() + 
  #historical min/max
  geom_line(data=data_env_hx, 
            aes(x=doy, y=!!rlang::sym(paste0(params$predictor_var2, "_mean")))) +
  #geom_ribbon(data=doymet, 
  #            aes(x=doy, ymin=min_var1, ymax=max_var1), alpha=0.3) +
  #observed
  geom_line(data=data_env %>% filter(year_epi == epiyear_request), 
            aes(x=doy, y=!!rlang::sym(params$predictor_var2)), 
            color="red", size=1) +
  ggtitle(paste(params$predictor_var2,
                max(data_env$year_epi, na.rm=TRUE),
                sep=" ")) +
  xlab("Day of the year") + ylab(params$predictor_var2)

grid.arrange(p_env_var1, p_env_var2, nrow=2)

#<<>> median values not directly
# plot1 <- ggplot() + geom_line(data=doymet, aes(x=doy, y=med_var1)) +
#   geom_ribbon(data=doymet, aes(x=doy, ymin=min_var1, ymax=max_var1), alpha=0.3) +
#   geom_line(data=thisyear, aes(x=doy, y=med_var1), color="red", size=1) +
#   xlab("Day of the year") + ylab(var1name) +
#   ggtitle(paste(var1name, 
#                 max(weather$year, na.rm=TRUE),
#                 sep=" "))
# plot2 <- ggplot() + geom_line(data=doymet, aes(x=doy, y=med_var2)) +
#   geom_ribbon(data=doymet, aes(x=doy, ymin=min_var2, ymax=max_var2), alpha=0.3) +
#   geom_line(data=thisyear, aes(x=doy, y=med_var2), color="red", size=1) +
#   ggtitle(paste(var2name,
#                 max(weather$year, na.rm=TRUE),
#                 sep=" ")) +
#   xlab("Day of the year") + ylab(var2name)
# grid.arrange(plot1, plot2, nrow=2)



```

## Human cases

```{r human_charts, fig.width=7, fig.height=3, echo=FALSE, include=TRUE}


# #quick & dirty V3 version just to look at <<>> DEV
# data_human_st <- data_combined %>% 
#   dplyr::group_by(week_epi) %>% 
#   dplyr::summarise(tot_any = sum(any_cases, na.rm = TRUE),
#                    .groups = 'drop')
# ggplot() + 
#   geom_line(data = data_human_st, 
#             aes(x=week_epi, y=tot_any)) +
#   ggtitle("TEMPORARY <<>> DEV") +
#   xlab("week number") + ylab("total districts positive") +
#   geom_vline(xintercept=human_wk_min, linetype=2) +
#   geom_vline(xintercept=human_wk_max, linetype=2) +
#   #week range should be full range
#   scale_x_continuous(breaks=seq(from=min(data_combined$week_epi),
#                                 to=max(data_combined$week_epi),
#                                 by=2)) +
#   theme_arbo_chart
# 

```

## Reference map

```{r reference_map, fig.width=7, fig.height=3, echo=FALSE, include=TRUE}


```

## Parameters

```{r params_pretty, include=TRUE, echo=FALSE}
#<<>>
# rem to remove override data and reg objs
# rem model list and names? 


```


```{r appendix_switch, echo = FALSE}
#child=if (params$create_appendix) 'ArboMAP_forecast_appendix.Rmd'
# Conditionally add the appendix child file (to be created) depending the T/F value of create_appendix 
#https://bookdown.org/yihui/rmarkdown-cookbook/child-document.html

# <<>> DEV look at epidemia report to see what, if anything, needs set up here. 

```
